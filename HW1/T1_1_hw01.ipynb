{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1\n",
    "\n",
    "\n",
    "## Readings: Lecture 02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "In lecture, we discussed minimization programs, but maximization programs are another possibility. Given a feasible region $\\mathcal{X}$ and a **reward** function $f:\\mathcal{X}\\rightarrow\\mathbb{R}$, a maximization program\n",
    "\n",
    "$$\n",
    "\\max f(x)\\text{ subject to } x\\in\\mathcal{X}\n",
    "$$\n",
    "\n",
    "seeks a **maximizer** of $f$ over $\\mathcal{X}$ (a point $x^\\ast$ satisfying $f(x)\\leq f(x^\\ast)$ for all $x\\in\\mathcal{X}$). Just as in minimization, we say that a minimizer of $f$ over $\\mathcal{X}$ is a **solution** to the program, or **solves** the program.\n",
    "\n",
    "For this problem, show that $x^\\ast$ is a solution to\n",
    "\n",
    "$$\n",
    "\\max f(x)\\text{ subject to } x\\in\\mathcal{X}\n",
    "$$\n",
    "\n",
    "if and only if $x^\\ast$ is a solution to\n",
    "\n",
    "$$\n",
    "\\min -f(x)\\text{ subject to } x\\in\\mathcal{X}.\n",
    "$$\n",
    "\n",
    "**Hint**: Recall that, to prove \"A if and only if B\", you need to prove two separate statements:\n",
    "1. If A, then B.\n",
    "2. If B, then A.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution to Problem 1\n",
    "\n",
    "Proof:\n",
    "\n",
    "**A$\\rightarrow$B:**\n",
    "\n",
    "We suppose $x^\\ast$ is a maximizer of $f$ over $\\mathcal{X}$, which means that for any $x\\in\\mathcal{X}$, $f(x^\\ast)\\geq f(x)$. So that leads to the assumption that for any $x\\in\\mathcal{X}$, $-f(x^\\ast)\\leq -f(x)$, which is just the same saying as $x^\\ast$ is a minimizer of $-f(x)$.\n",
    "\n",
    "**B$\\rightarrow$A:**\n",
    "\n",
    "We suppose $x^\\ast$ is a minimizer of $-f$ over $\\mathcal{X}$, which means that for any $x\\in\\mathcal{X}$, $-f(x^\\ast)\\leq -f(x)$. So that leads to the assumption that for any $x\\in\\mathcal{X}$, $f(x^\\ast)\\geq f(x)$, which is just the same saying as $x^\\ast$ is a maximizer of $f(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "The fact that one can replace any maximization problem with a minimization problem means that we only need to consider minimization problems in general. This is called the **reflection principle** because $-f(x)$ reflects $f(x)$ across the $x$ axis. Because of the reflection principle, when we talk about optimization programs in this course, we will almost exclusively be discussing minimization programs. \n",
    "\n",
    "This also illustrates how it sometimes helps to modify a function $f$ to find a solution to a program. For two programs\n",
    "$$\n",
    "(P_1):\\:\\min f(x)\\text{ subject to } x\\in\\mathcal{X}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "(P_2):\\:\\min g(x)\\text{ subject to } x\\in\\mathcal{X},\n",
    "$$\n",
    "whenever it holds that $x^\\ast$ solves $P_1$ if and only $x^\\ast$ solves $P_2$, we say that the programs $P_1$ and $P_2$ are **equivalent**. We also say that these programs are equivalent if this statement holds, but either of the $\\min$'s are replaced with $\\max$'s. \n",
    "\n",
    "For example, this means that $\\max f(x)$ and $\\min -f(x)$ are equivalent programs by the reflection principle.\n",
    "\n",
    "### Part A\n",
    "\n",
    "For this part, show that for any **monotonically decreasing** or **order-reversing** function $h:\\mathbb{R}\\rightarrow\\mathbb{R}$ ($h(x)<h(y)$ whenever $y<x$),\n",
    "\n",
    "$$\n",
    "\\max f(x)\\text{ subject to } x\\in\\mathcal{X}\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "\\min h(f(x))\\text{ subject to }x\\in\\mathcal{X}\n",
    "$$\n",
    "\n",
    "are equivalent programs. \n",
    "\n",
    "### Part B\n",
    "\n",
    "Also show that, for any **monotonically increasing** or **order-preserving** function $\\widetilde{h}:\\mathbb{R}\\rightarrow\\mathbb{R}$ ($h(x)<h(y)$ whenever $y<x$),\n",
    "\n",
    "$$\n",
    "\\min f(x)\\text{ subject to }x\\in\\mathcal{X}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\min \\widetilde{h}(f(x))\\text{ subject to }x\\in\\mathcal{X}\n",
    "$$\n",
    "\n",
    "are equivalent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution to Problem 2\n",
    "\n",
    "### Part A\n",
    "\n",
    "Proof:\n",
    "\n",
    "Necessary Condition:\n",
    "\n",
    "We suppose that $x^\\ast$ is a maximizer of $f(x)$ for $x\\in\\mathcal{X}$, which means that for any $x\\in\\mathcal{X}$, we have $f(x^\\ast)\\geq f(x)$. In the case of $f(x^\\ast)=f(x)$, we have $h(f(x^\\ast))=h(f(x))$. In the case of $f(x^\\ast)>f(x)$, we have $h(f(x^\\ast))<h(f(x))$ according to the monotonical decreasing feature. In conclusion, we have $h(f(x^\\ast))\\leq h(f(x))$ for any $x\\in\\mathcal{X}$.\n",
    "\n",
    "Sufficient Condition:\n",
    "\n",
    "We suppose that $x^\\ast$ is a minimizer of $h(f(x))$ for $x\\in\\mathcal{X}$, which means that for any $x\\in\\mathcal{X}$, we have $h(f(x^\\ast))\\leq h(f(x))$. In the case of $h(f(x^\\ast))=h(f(x))$, if $f(x^\\ast)\\neq f(x)$, we will have $h(f(x^\\ast))> h(f(x))$ or $ h(f(x^\\ast))< h(f(x))$ according to the monotonical feature. So $f(x^\\ast)=f(x)$. In the case of $h(f(x^\\ast))<h(f(x))$, if $f(x^\\ast)\\leq f(x)$,we have $h(f(x^\\ast))\\geq h(f(x))$ according to the monotonical decreasing feature. So $f(x^\\ast)<f(x)$. In conclusion, we have $(f(x^\\ast))\\geq (f(x))$ for any $x\\in\\mathcal{X}$.\n",
    "\n",
    "### Part B\n",
    "\n",
    "Proof:\n",
    "\n",
    "Necessary Condition:\n",
    "\n",
    "We suppose that $x^\\ast$ is a minimizer of $f(x)$ for $x\\in\\mathcal{X}$, which means that for any $x\\in\\mathcal{X}$, we have $f(x^\\ast)\\leq f(x)$. In the case of $f(x^\\ast)=f(x)$, we have $\\widetilde{h}(f(x^\\ast))=\\widetilde{h}(f(x))$. In the case of $f(x^\\ast)<f(x)$, we have $\\widetilde{h}(f(x^\\ast))<\\widetilde{h}(f(x))$ according to the monotonical decreasing feature. In conclusion, we have $\\widetilde{h}(f(x^\\ast))\\leq \\widetilde{h}(f(x))$ for any $x\\in\\mathcal{X}$.\n",
    "\n",
    "Sufficient Condition:\n",
    "\n",
    "We suppose that $x^\\ast$ is a minimizer of $\\widetilde{h}(f(x))$ for $x\\in\\mathcal{X}$, which means that for any $x\\in\\mathcal{X}$, we have $\\widetilde{h}(f(x^\\ast))\\leq \\widetilde{h}(f(x))$. In the case of $\\widetilde{h}(f(x^\\ast))=\\widetilde{h}(f(x))$, if $f(x^\\ast)\\neq f(x)$, we will have $\\widetilde{h}(f(x^\\ast))> \\widetilde{h}(f(x))$ or $ \\widetilde{h}(f(x^\\ast))< \\widetilde{h}(f(x))$ according to the monotonical feature. So $f(x^\\ast)=f(x)$. In the case of $\\widetilde{h}(f(x^\\ast))<\\widetilde{h}(f(x))$, if $f(x^\\ast)\\geq f(x)$,we have $\\widetilde{h}(f(x^\\ast))\\geq \\widetilde{h}(f(x))$ according to the monotonical increasing feature. So $f(x^\\ast)<f(x)$. In conclusion, we have $(f(x^\\ast))\\leq (f(x))$ for any $x\\in\\mathcal{X}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3:\n",
    "\n",
    "Linear regression is often introduced by setting\n",
    "\n",
    "$$\n",
    "y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i\n",
    "$$\n",
    "\n",
    "where each $\\varepsilon_i$ is an unknown number drawn from a normal distribution with mean $0$ and known variance $\\sigma^2$. That is, the p.d.f. of $\\varepsilon_i$ is \n",
    "\n",
    "$$\n",
    "\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\varepsilon_i^2/2\\sigma^2}.\n",
    "$$\n",
    "\n",
    "Because of the first inequality, we can rewrite this p.d.f. as the **likelihood** of $\\beta_0,\\beta_1$ given the data $(x_i, y_i)$:\n",
    "\n",
    "$$\n",
    "\\ell(\\beta_0,\\beta_1; (x_i, y_i)) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(y_i-\\beta_0-\\beta_1x_i)^2}{2\\sigma^2}}.\n",
    "$$\n",
    "\n",
    "Since the $\\varepsilon_i$'s are independent the p.d.f. of all the errors is simple the product of p.d.f.'s, and hence the likelihood of observing the data set is simply the product of likelihoods:\n",
    "\n",
    "$$\n",
    "\\ell(\\beta_0,\\beta_1; \\{(x_i, y_i)\\}_{i=1}^N) = \\prod_{i=1}^N\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(y_i-\\beta_0-\\beta_1x_i)^2}{2\\sigma^2}}.\n",
    "$$\n",
    "\n",
    "The **maximum likelihood principle** states that we should find $\\beta_0$ and $\\beta_1$ which maximize the likelihood function $\\ell$.\n",
    "\n",
    "Show that\n",
    "$$\n",
    "\\max \\ell(\\beta_0, \\beta_1)\\text{ subject to }(\\beta_0,\\beta_1)\\in\\mathbb{R}^2\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\min \\frac{1}{N}\\Vert y-X\\beta\\Vert^2\\text{ subject to }\\beta\\in\\mathbb{R}^2\n",
    "$$\n",
    "are equivalent programs. Here, we have used $y$ and $X$ from the lecture, and we have suppressed the data dependency of $\\ell$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution to Problem 3\n",
    "\n",
    "Proof:\n",
    "\n",
    "Since $log(x)$ is a monotonical increasing function over $(0,+\\infty)$ and the p.d.f. value is always above 0, we can apply the conclusion of **problem 2 part B** with $\\widetilde{h}(x)=log(x)$ to this problem. What we have here for $\\widetilde{h}(f(x))$ is: $$-\\frac{N log(2 \\pi \\sigma^2)}{2} - \\sum_{i=1}^N\\frac{(y_i-\\beta_0-\\beta_1x_i)^2}{2\\sigma^2}$$We can ignore the constant cofficient as well as the positive factor and apply conclusion of **problem 1** to receive a equivalent problem:$$min\\sum_{i=1}^N(y_i-\\beta_0-\\beta_1x_i)^2\\ subject\\ to\\ (\\beta_0,\\beta_1)\\in R^2$$If we use the expression of:$$X=\\begin{bmatrix}{1}&{x_1}\\\\{1}&{x_2}\\\\{\\vdots}&{\\vdots}\\\\{1}&{x_N}\\\\\\end{bmatrix}\\ \\beta=\\begin{bmatrix}{\\beta_0}\\\\{\\beta_1}\\\\\\end{bmatrix}$$Then we have the equivalent problem:$$min\\|y-X\\beta\\|^2\\ subject\\ to\\ \\beta\\in R^2$$\n",
    "We notice that by adding a factor of $\\frac{1}{N}$, we can achieve what we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "Prove the parallelogram identity:\n",
    "\n",
    "$$\n",
    "x^Ty = \\frac{1}{4}\\left(\\Vert x+y\\Vert^2-\\Vert x-y\\Vert^2\\right)\n",
    "$$\n",
    "\n",
    "for all $x,y\\in\\mathbb{R}^d$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution to Problem 4\n",
    "\n",
    "Proof:\n",
    "\n",
    "$$\\frac{1}{4}(\\|x+y\\|^2-\\|x-y\\|^2)$$$$=\\frac{1}{4}((x+y)^T(x+y)-(x-y)^T(x-y))$$$$=\\frac{1}{4}(x^Tx+2x^Ty+y^Ty-x^Tx+2x^Ty-y^Ty)\\ (By\\ applying\\ features\\ of\\ the\\ inner\\ product)$$$$=x^Ty$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5\n",
    "\n",
    "Another benefit of our reformulation of the $SSE$ function is that it is now dimension independent. That is, if we are given data $\\{(x_i, y_i)\\}_{i=1}^N$ with $x_i\\in\\mathbb{R}^d$ and $y_i\\in\\mathbb{R}$ for each $i=1,\\ldots, N$ and we form the higher-order $SSE$ function by defining\n",
    "\n",
    "$$\n",
    "SSE(\\beta_0,\\beta_1,\\ldots, \\beta_d) = \\sum_{i=1}^N(y_i-\\beta_0-\\beta_1x_{i,1}-\\beta_2x_{i,2}-\\cdots-\\beta_d x_{i,d})^2\n",
    "$$\n",
    "\n",
    "with $x_{i,j}$ the $j$th entry of $x_i$, then\n",
    "\n",
    "$$\n",
    "SSE(\\beta) = \\Vert y - X\\beta\\Vert^2\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "y=\\begin{pmatrix}y_1\\\\ y_2\\\\\\vdots\\\\ y_N\\end{pmatrix},\\:\\beta=\\begin{pmatrix}\\beta_0\\\\\\beta_1\\\\\\vdots\\\\\\beta_d\n",
    "\\end{pmatrix},\\text{ and } X=\\begin{pmatrix} 1 & x_{1, 1} & x_{1,2} & \\cdots & x_{1,d}\\\\1 & x_{2, 1} & x_{2,2} & \\cdots & x_{2,d}\\\\\\vdots & \\vdots& \\vdots &\\ddots & \\vdots\\\\1 & x_{N, 1} & x_{N,2} & \\cdots & x_{N,d}\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "### Part A\n",
    "\n",
    "Modify the code from class to compute the $\\beta^\\ast\\in\\mathbb{R}^{14}$ (the $0$th entry is the constant $\\beta_0$ and the other entries correpond to the $13$ features of the dataset) solving the least squares problem. Print the $\\beta^\\ast$'s and discuss your findings.\n",
    "\n",
    "### Part B\n",
    "\n",
    "Numerically verify that the $\\beta^\\ast$ satisfies the normal equations. That is, use python code to show that $X^TX\\beta^\\ast - X^T y\\approx 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Solution to Problem 5\n",
    "\n",
    "### Part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _boston_dataset:\n",
      "\n",
      "Boston house prices dataset\n",
      "---------------------------\n",
      "\n",
      "**Data Set Characteristics:**  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      ".. topic:: References\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston # Import the Boston housing dataset loader\n",
    "\n",
    "boston = load_boston() # Instantiate the python map object containing the Boston housing dataset and information\n",
    "print(boston.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13)\n",
      "[[2.7310e-02 0.0000e+00 7.0700e+00 0.0000e+00 4.6900e-01 6.4210e+00\n",
      "  7.8900e+01 4.9671e+00 2.0000e+00 2.4200e+02 1.7800e+01 3.9690e+02\n",
      "  9.1400e+00]\n",
      " [2.7290e-02 0.0000e+00 7.0700e+00 0.0000e+00 4.6900e-01 7.1850e+00\n",
      "  6.1100e+01 4.9671e+00 2.0000e+00 2.4200e+02 1.7800e+01 3.9283e+02\n",
      "  4.0300e+00]\n",
      " [3.2370e-02 0.0000e+00 2.1800e+00 0.0000e+00 4.5800e-01 6.9980e+00\n",
      "  4.5800e+01 6.0622e+00 3.0000e+00 2.2200e+02 1.8700e+01 3.9463e+02\n",
      "  2.9400e+00]\n",
      " [6.9050e-02 0.0000e+00 2.1800e+00 0.0000e+00 4.5800e-01 7.1470e+00\n",
      "  5.4200e+01 6.0622e+00 3.0000e+00 2.2200e+02 1.8700e+01 3.9690e+02\n",
      "  5.3300e+00]\n",
      " [2.9850e-02 0.0000e+00 2.1800e+00 0.0000e+00 4.5800e-01 6.4300e+00\n",
      "  5.8700e+01 6.0622e+00 3.0000e+00 2.2200e+02 1.8700e+01 3.9412e+02\n",
      "  5.2100e+00]]\n"
     ]
    }
   ],
   "source": [
    "x=boston.data[:,:]\n",
    "print(x.shape)\n",
    "print(x[1:6,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506,)\n"
     ]
    }
   ],
   "source": [
    "y=boston.target\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.64594884e+01 -1.08011358e-01  4.64204584e-02  2.05586264e-02\n",
      "  2.68673382e+00 -1.77666112e+01  3.80986521e+00  6.92224640e-04\n",
      " -1.47556685e+00  3.06049479e-01 -1.23345939e-02 -9.52747232e-01\n",
      "  9.31168327e-03 -5.24758378e-01]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # Import numerical python routines\n",
    "\n",
    "# One nuisance with numpy is that one may have to coerce the shape of an array to process it\n",
    "one = np.ones((506,1))\n",
    "x = np.reshape(x, (506, 13))\n",
    "\n",
    "X = np.concatenate((one, x), axis=1) # Form the design matrix for the NO_2 levels \n",
    "beta = np.linalg.lstsq(X, y, rcond=None)[0] # Use the linear algebra least squares routine to compute the beta's\n",
    "print(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions:\n",
    "\n",
    "Different cofficients have different scales. I think there are two main reasons for the phenomenon. First, different components of Vector $x$ have different numerical scales. Second, different components of Vector $x$ have different degrees of contribution to the result $y$. What we care about most is the second point. So maybe a good way to reduce the influence of the first point is to normalize the Vector $x$ using some kind of methods to make sure they are of the same numerical scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.45696821e-12  5.09317033e-11 -1.16415322e-10 -1.01863407e-10\n",
      " -1.47792889e-12 -3.63797881e-12 -4.36557457e-11 -6.98491931e-10\n",
      " -3.63797881e-11 -5.82076609e-11 -1.86264515e-09 -1.45519152e-10\n",
      " -1.86264515e-09 -1.01863407e-10]\n"
     ]
    }
   ],
   "source": [
    "temp=np.dot(X,beta.T)\n",
    "epsilon=np.dot(X.T,temp)-np.dot(X.T,y)\n",
    "print(epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions:\n",
    "\n",
    "We can see the matrix epsilon is nearly the zero matrix of its scale. In fact, when we apply the function of **np.linalg.lstsq**, a stable numerical method to solve the problem of $X^TX\\beta^\\ast - X^T y= 0$ is used. The bias is mainly due to system error of the method and the error during calculation."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
