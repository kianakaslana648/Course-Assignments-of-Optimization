{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 10\n",
    "\n",
    "## Readings: Lecture 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "Use the code from Lecture 10 to generate a dataset $\\{x^{(i)}\\}_{i=0}^{19}\\subset\\mathbb{R}^2$ with labels $\\{y^{(i)}\\}_{i=0}^{19}\\in\\{-1,1\\}$ such that $x^{(i)}$ is drawn uniformly from the unit disc centered at $(-2, -2)$ and $y^{(i)}=-1$for $i=0,\\ldots, 9$, and $x^{(i)}$ is drawn uniformly from the unit disc centered at $(2, 2)$ and $y^{(i)}=+1$ for $i=10,\\ldots, 19$. \n",
    "\n",
    "### Part A\n",
    "Code up a Phase I method to find an interior point for the (3D) program\n",
    "$$\n",
    "\\min_{{\\bf v}\\in\\mathbb{R}^2, b\\in\\mathbb{R}} \\frac{1}{2}\\Vert {\\bf v}\\Vert^2\n",
    "$$\n",
    "subject to\n",
    "$$\n",
    "h_i({\\bf v},b)\\leq 0\\text{ for }i=0,1,\\ldots, 19.\n",
    "$$\n",
    "where $h_i({\\bf v},b)=1- y^{(i)}({\\bf v}^Tx^{(i)}-b)$. That is, use the log barrier method to solve the 4D program\n",
    "$$\n",
    "\\min_{{\\bf v}\\in\\mathbb{R}^2, b\\in\\mathbb{R}, z\\in\\mathbb{R}} \\widetilde{f}({\\bf v}, b, z)\\text{ subject to } \\widetilde{h}_i({\\bf v},b, z)\\leq 0\n",
    "$$\n",
    "where $\\widetilde{f}({\\bf v}, b, z)=z$ and $\\widetilde{h}_i({\\bf v},b, z)=1- y^{(i)}({\\bf v}^Tx^{(i)}-b)-z$, but stop as soon as you find an iterate $({\\bf v}^{(k)}, b^{(k)}, z^{(k)})$ such that $z^{(k)}<0$ (which then implies that $({\\bf v}^{(k)}, b^{(k)})$ is an interior point). Initialize the Phase I method with the data ${\\bf v}^{(0)}=(-20, 20)$, $b^{(0)}=10$, and determine $z^{(0)}$ (in your code) such that $({\\bf v}^{(0)}, b^{(0)}, z^{(0)})$ is an interior point of the 4D convex program.\n",
    "\n",
    "### Part B\n",
    "\n",
    "Use the $({\\bf v}^{(k)}, b^{(k)})$ from part (a) to initialize Phase II for your generated data. That is, use this interior point to initialize the log barrier method for the 3D program. Use $3$ centering steps, $M=10$, and $5$ iterations in the outer loop with $2$ inner loop iterations each. For each backtracking step, use Newton search directions, $\\alpha=0.1$, and $\\beta=0.5$.\n",
    "\n",
    "### Part C\n",
    "Derive and code up the primal-dual algorithm for this program. Use the $({\\bf v}^{(k)}, b^{(k)})$ from part (a) to initialize the primal-dual algorithm. Take $10$ steps with $\\nu=10$. \n",
    "\n",
    "### Part D\n",
    "Compare the answers you get from parts (b) and (c), and provide a simultaneous plot of your data, the separating line corresponding to the interior point $({\\bf v}^{(k)}, b^{(k)})$ from part (a), and the two separating lines from parts (b) and (c).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution to Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def random_circle(N):\n",
    "    x = np.reshape(rd.randn(N*2), (N, 2))\n",
    "    for i in range(N):\n",
    "        x[i,:] = x[i,:]/np.sqrt(np.sum(x[i,:]**2))\n",
    "    return x\n",
    "\n",
    "def random_radius(N, R=1):\n",
    "    r = rd.rand(N)\n",
    "    return R*np.sqrt(r) # This ensures uniform sampling from the disc\n",
    "    \n",
    "\n",
    "def random_disc(N, mu=[0,0], R=1):\n",
    "    x = random_circle(N)\n",
    "    r = random_radius(N, R=R)\n",
    "    for i in range(N):\n",
    "        x[i, :] = r[i] * x[i, :] + mu\n",
    "    return x\n",
    "\n",
    "N=10\n",
    "\n",
    "X = np.zeros((20,2))\n",
    "Y = np.zeros(20)\n",
    "X[:10,] = random_disc(N, mu=[-2, -2])\n",
    "X[10:,] = random_disc(N, mu=[2, 2])\n",
    "Y[:10]=1\n",
    "Y[10:]=-1\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1])\n",
    "plt.plot([-4, 4], [0, 0])\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def backtracking3(v0, b0, z0, dv, db, dz, f, df0, alpha=0.2, beta=0.8):\n",
    "    '''\n",
    "    Backtracking for general functions with illustrations\n",
    "    :param x0: Previous point from backtracking, or initial guess\n",
    "    :param dx: Incremental factor for updating x0\n",
    "    :param f: Objective function\n",
    "    :param df0: Gradient of f at x0\n",
    "    :param alpha: Sloping factor of stopping criterion\n",
    "    :param beta: \"Agressiveness\" parameter for backtracking steps\n",
    "    :param verbose: Boolean for providing plots and data\n",
    "    :return: x1, the next iterate in backtracking\n",
    "    '''\n",
    "\n",
    "    # Note that the definition below requires that dx and df0 have the same shape\n",
    "    delta = alpha * (np.sum(dv * df0[:2])+ db*df0[2]+dz*df0[3]) # A general, but memory intensive inner product\n",
    "    \n",
    "    t = 1 # Initialize t=beta^0\n",
    "    f0 = f(v0, b0, z0) # Evaluate for future use\n",
    "    v = v0 + dv # Initialize x_{0, inner}\n",
    "    b = b0 + db\n",
    "    z = z0 + dz\n",
    "    fx = f(v,b,z)\n",
    "    \n",
    "    while (not np.isfinite(fx)) or f0 + delta * t < fx:\n",
    "        t = beta * t\n",
    "        v = v0 + t * dv\n",
    "        b = b0 + t * db\n",
    "        z = z0 + t * dz\n",
    "        if(z < 0):\n",
    "            break\n",
    "        fx = f(v,b,z)\n",
    "    ###################################### \n",
    "\n",
    "    return v,b,z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two steps of the log barrier method\n",
    "\n",
    "### 0,1,2-order derivatives of original function\n",
    "fun = lambda v,b,z: z\n",
    "dfun = lambda v,b,z: np.array([0,0,0,1])\n",
    "Hfun = lambda v,b,z: np.zeros((4,4))\n",
    "\n",
    "### 0,1,2-order derivatives of log part\n",
    "logh = lambda v,b,z: np.sum(np.log((X@v-b)*Y+z-1))\n",
    "dlogh = lambda v,b,z: np.array([np.sum(Y*X[:,0]*((X@v-b)*Y+z-1)**(-1)), np.sum(Y*X[:,1]*((X@v-b)*Y+z-1)**(-1)), \\\n",
    "                               np.sum(-Y*((X@v-b)*Y+z-1)**(-1)), np.sum(((X@v-b)*Y+z-1)**(-1))])\n",
    "Hlogh = lambda v,b,z: np.array([[np.sum(-(X[:,0]**2)*((X@v-b)*Y+z-1)**(-2)),np.sum(-(X[:,0]*X[:,1])*((X@v-b)*Y+z-1)**(-2)),\\\n",
    "                                np.sum(X[:,0]*((X@v-b)*Y+z-1)**(-2)), np.sum(-Y*X[:,0]*((X@v-b)*Y+z-1)**(-2))],\\\n",
    "                                \\\n",
    "                                [np.sum(-(X[:,0]*X[:,1])*((X@v-b)*Y+z-1)**(-2)),np.sum(-(X[:,1]**2)*((X@v-b)*Y+z-1)**(-2)),\\\n",
    "                                np.sum(X[:,1]*((X@v-b)*Y+z-1)**(-2)), np.sum(-Y*X[:,1]*((X@v-b)*Y+z-1)**(-2))],\\\n",
    "                                \\\n",
    "                                [np.sum(X[:,0]*((X@v-b)*Y+z-1)**(-2)),np.sum(X[:,1]*((X@v-b)*Y+z-1)**(-2)),\\\n",
    "                                np.sum(-((X@v-b)*Y+z-1)**(-2)), np.sum(Y*((X@v-b)*Y+z-1)**(-2))],\\\n",
    "                                \\\n",
    "                                [np.sum(-Y*X[:,0]*((X@v-b)*Y+z-1)**(-2)),np.sum(-Y*X[:,1]*((X@v-b)*Y+z-1)**(-2)),\\\n",
    "                                np.sum(Y*((X@v-b)*Y+z-1)**(-2)), np.sum(-((X@v-b)*Y+z-1)**(-2))]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.34868309400816\n"
     ]
    }
   ],
   "source": [
    "v0=np.array([-20,20])\n",
    "b0=10\n",
    "z0=max(1-Y*(X@v0-b0))+1\n",
    "print(z0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-30.646379     8.13582334]\n",
      "8.306937566605363\n",
      "-4.118664307639923\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.2\n",
    "beta = 0.8\n",
    "t=1\n",
    "\n",
    "v=v0\n",
    "b=b0\n",
    "z=z0\n",
    "\n",
    "for i in range(5):\n",
    "    lb = lambda v,b,z: fun(v,b,z) - t*logh(v,b,z)\n",
    "    dlb = lambda v,b,z: dfun(v,b,z) - t*dlogh(v,b,z)\n",
    "    for j in range(10):\n",
    "        v,b,z = backtracking3(v, b, z, -dlb(v,b,z)[:2], -dlb(v,b,z)[2], -dlb(v,b,z)[3], lb,\\\n",
    "                              dlb(v,b,z), alpha=alpha, beta=beta)\n",
    "    if(z < 0):\n",
    "        break\n",
    "    t=t/10\n",
    "print(v)\n",
    "print(b)\n",
    "print(z)\n",
    "v0=v\n",
    "b0=b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtracking2(v0, b0, dv, db, f, df0, alpha=0.2, beta=0.8):\n",
    "    '''\n",
    "    Backtracking for general functions with illustrations\n",
    "    :param x0: Previous point from backtracking, or initial guess\n",
    "    :param dx: Incremental factor for updating x0\n",
    "    :param f: Objective function\n",
    "    :param df0: Gradient of f at x0\n",
    "    :param alpha: Sloping factor of stopping criterion\n",
    "    :param beta: \"Agressiveness\" parameter for backtracking steps\n",
    "    :param verbose: Boolean for providing plots and data\n",
    "    :return: x1, the next iterate in backtracking\n",
    "    '''\n",
    "\n",
    "    # Note that the definition below requires that dx and df0 have the same shape\n",
    "    delta = alpha * (np.sum(dv * df0[:2])+ db * df0[2]) # A general, but memory intensive inner product\n",
    "    \n",
    "    t = 1 # Initialize t=beta^0\n",
    "    f0 = f(v0, b0) # Evaluate for future use\n",
    "    v = v0 + dv # Initialize x_{0, inner}\n",
    "    b = b0 + db\n",
    "    fx = f(v,b)\n",
    "    \n",
    "    while (not np.isfinite(fx)) or f0 + delta * t < fx:\n",
    "        t = beta * t\n",
    "        v = v0 + t * dv\n",
    "        b = b0 + t * db\n",
    "        fx = f(v,b)\n",
    "    ###################################### \n",
    "\n",
    "    return v,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two steps of the log barrier method\n",
    "\n",
    "### 0,1,2-order derivatives of original function\n",
    "fun = lambda v,b: np.linalg.norm(v)**2/2\n",
    "dfun = lambda v,b: np.array([v[0],v[1],0])\n",
    "Hfun = lambda v,b: np.array([[1,0,0],[0,1,0],[0,0,0]])\n",
    "\n",
    "### 0,1,2-order derivatives of log part\n",
    "logh = lambda v,b: np.sum(np.log((X@v-b)*Y-1))\n",
    "dlogh = lambda v,b: np.array([np.sum(Y*X[:,0]*((X@v-b)*Y-1)**(-1)), np.sum(Y*X[:,1]*((X@v-b)*Y-1)**(-1)), \\\n",
    "                               np.sum(-Y*((X@v-b)*Y-1)**(-1))])\n",
    "Hlogh = lambda v,b: np.array([[np.sum(-(X[:,0]**2)*((X@v-b)*Y-1)**(-2)),np.sum(-(X[:,0]*X[:,1])*((X@v-b)*Y-1)**(-2)),\\\n",
    "                                np.sum(X[:,0]*((X@v-b)*Y-1)**(-2))],\\\n",
    "                                \\\n",
    "                                [np.sum(-(X[:,0]*X[:,1])*((X@v-b)*Y-1)**(-2)),np.sum(-(X[:,1]**2)*((X@v-b)*Y-1)**(-2)),\\\n",
    "                                np.sum(X[:,1]*((X@v-b)*Y-1)**(-2))],\\\n",
    "                                \\\n",
    "                                [np.sum(X[:,0]*((X@v-b)*Y-1)**(-2)),np.sum(X[:,1]*((X@v-b)*Y-1)**(-2)),\\\n",
    "                                np.sum(-((X@v-b)*Y-1)**(-2))]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.31112028 -0.30543208]\n",
      "0.09134161514974713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\learn\\lib\\site-packages\\ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in log\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.1\n",
    "beta = 0.5\n",
    "t=1\n",
    "\n",
    "v=v0\n",
    "b=b0\n",
    "\n",
    "for i in range(5):\n",
    "    lb = lambda v,b: fun(v,b) - t*logh(v,b)\n",
    "    dlb = lambda v,b: dfun(v,b) - t*dlogh(v,b)\n",
    "    hlb = lambda v,b: Hfun(v,b) - t * Hlogh(v,b)\n",
    "    nt = lambda v,b: np.linalg.solve(hlb(v,b),dlb(v,b))\n",
    "    for j in range(2):\n",
    "        v,b = backtracking2(v, b, -nt(v,b)[:2], -nt(v,b)[2], lb,\\\n",
    "                              dlb(v,b), alpha=alpha, beta=beta)\n",
    "    t=t/10\n",
    "print(v)\n",
    "print(b)\n",
    "v1=v\n",
    "b1=b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.31226181 -0.29954512]\n",
      "0.09349597790128443\n",
      "[[1.79546535e-04]\n",
      " [2.20235312e-04]\n",
      " [1.53187849e-02]\n",
      " [8.28981050e-04]\n",
      " [1.25949315e-04]\n",
      " [1.39907090e-04]\n",
      " [2.74152138e-04]\n",
      " [1.61858587e-04]\n",
      " [7.57244803e-02]\n",
      " [1.46856049e-04]\n",
      " [4.56521291e-04]\n",
      " [9.14671381e-02]\n",
      " [8.40094262e-05]\n",
      " [6.93448160e-05]\n",
      " [1.13977639e-04]\n",
      " [7.93465852e-05]\n",
      " [4.23275791e-04]\n",
      " [6.54201206e-05]\n",
      " [1.62981205e-04]\n",
      " [1.98736338e-04]]\n"
     ]
    }
   ],
   "source": [
    "def l2(x):\n",
    "    return np.sqrt(np.sum(x**2))\n",
    "\n",
    "f = lambda v,b: np.reshape(np.sum(v**2)/2, (1, 1)) # Reshape coerces dimension for block\n",
    "df = lambda v,b: np.reshape(np.array([v[0],v[1],0]),(3,1))\n",
    "d2f = lambda v,b: np.array([[1,0,0],[0,1,0],[0,0,0]])\n",
    "\n",
    "conh = lambda v,b: np.reshape(1-(X@v-b)*Y, (20, 1))\n",
    "condh = lambda v,b: np.array([-X[:,0]*Y,-X[:,1]*Y,Y]).T\n",
    "\n",
    "phi = lambda v, b, mu, t:  np.concatenate((df(v,b)+(condh(v,b).T)@mu, -mu*conh(v,b)-1/t))\n",
    "dphi = lambda v, b, mu: np.reshape(np.block([[d2f(v,b), condh(v,b).T], \\\n",
    "                                             [-np.diag(np.reshape(mu,20))@condh(v,b),\\\n",
    "                                             np.diag(-np.reshape([conh(v,b)],20))]]), (d+m, d+m)) \n",
    "# block is new in numpy 1.13.0\n",
    "\n",
    "def diagnostic(d, x, mu, t, dx_mu):\n",
    "    n = 100\n",
    "    s = np.linspace(0, 1.0, n)\n",
    "    vals = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        vals[i] = l2(phi(x + s[i]*dx_mu[:d], mu + s[i]*dx_mu[d:], t))\n",
    "        \n",
    "    plt.plot(s, vals)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "num_iter = 10\n",
    "d = 3 # dimension of the program\n",
    "m = 20 # number of inequality constraints\n",
    " # initial values for x_1, x_2 -- dimension is coerced to ensure column vector format\n",
    "mu0 = -1/conh(v0,b0) # initial value for mu\n",
    "\n",
    "\n",
    "nu = 10 # Interior point scaling parameter\n",
    "\n",
    "# Backtracking parameters\n",
    "alpha = 0.1\n",
    "beta = 0.5\n",
    "\n",
    "eta = lambda v, b, mu: -np.sum(conh(v,b)*mu) # Computation of surrogate duality gap\n",
    "\n",
    "v=v0\n",
    "b=b0\n",
    "mu = mu0\n",
    "for i in range(num_iter):\n",
    "    t = nu * m / eta(v, b, mu)\n",
    "    phi0 = phi(v,b,mu,t)\n",
    "    norm_phi0 = l2(phi0)\n",
    "    \n",
    "    # Compute the Newton search direction\n",
    "\n",
    "    dx_mu = np.linalg.solve(dphi(v, b, mu), -phi0)\n",
    "    \n",
    "    # Initialize backtracking\n",
    "    s = 1 # using s for backtracking parameter since t is taken\n",
    "    yv = v + np.reshape(s*dx_mu[:2],2)\n",
    "    yb = b + s*dx_mu[2,0]\n",
    "    ymu = mu + s*dx_mu[d:]\n",
    "    hyx = conh(yv, yb)\n",
    "    phiy = phi(yv, yb, ymu, t)\n",
    "    norm_phiy = l2(phiy)\n",
    "    \n",
    "    #diagnostic(d, x, mu, t, dx_mu)\n",
    "    \n",
    "    while 0 < ymu[ymu<=0].size or 0 < hyx[0<=hyx].size or (1-alpha*s)*norm_phi0 < norm_phiy:\n",
    "        s = beta * s\n",
    "        yv = v + np.reshape(s*dx_mu[:2],2)\n",
    "        yb = b + s*dx_mu[2,0]\n",
    "        ymu = mu + s*dx_mu[d:]\n",
    "        hyx = conh(yv, yb)\n",
    "        phiy = phi(yv, yb, ymu, t)\n",
    "        norm_phiy = l2(phiy)\n",
    "        \n",
    "    v = yv\n",
    "    b = yb\n",
    "    mu = ymu\n",
    "    \n",
    "print(v)\n",
    "print(b)\n",
    "print(mu)\n",
    "v2=v\n",
    "b2=b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19008458162167846 0.18723471459789118\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEGCAYAAABsLkJ6AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAna0lEQVR4nO3deXxU1f3/8dcnG4RFgixSFkUtoigIGhQEBBIQkVXUCoqKa7VicfliAduirdZ9ba2VuhQURBREcUMumyxFAUEREQVcADdQIksCZDm/Pwb8KU1Clpk5k5n38/HgkWZmcu/7kcZ5zz333HPNOYeIiCSeJN8BRETEDxWAiEiCUgGIiCQoFYCISIJSAYiIJKgU3wHKo379+q558+a+Y4iIVCnLly/f6pxrcODjVaoAmjdvzrJly3zHEBGpUszsi+Ie1xCQiEiCUgGIiCQoFYCISIJSAYiIJCgVgIhIglIBiIgkKBWAiEiCUgGIxCLn4OabYelS30kkjqkARGLR6tVw773w4Ye+k0gcUwGIxKIgCH3NzvabQ+KaCkAkFgUBHHMMHH647yQSx1QAIrEmPx/mzYMePXwnkTinAhCJNe+8A7t2qQAk4lQAIrEmCCApCbp1851E4pwKQCTWBAFkZkLdur6TSJxTAYjEku3bYckSDf9IVKgARGLJ229DYaEKQKJCBSASS4IA0tOhY0ffSSQBqABEYkkQQJcuUL267ySSAFQAIrHi669DS0Bo+EeiRAUgEitmzw59VQFIlKgARGJFEEC9enDiib6TSILwXgBmlmxmK8zsVd9ZRLxxLlQA2dmhi8BEoiAW/tJGAGt8hxDxau1a2LxZq39KVHktADNrCvQBnvCZQ8S7/cs/a/xfosj3EcBDwM1AUUkvMLOrzGyZmS3bsmVL1IKJRNXs2XDkkXDUUb6TSALxVgBm1hf4zjm3vLTXOefGOecynXOZDRo0iFI6kSgqKIC5c/XpX6LO5xFAJ6C/mX0OTAayzOxZj3kSwsMPw6hRoXOOEiOWL4cff1QBSNR5KwDn3GjnXFPnXHNgMDDHOTfUV55E4Bx8vLaIu++Gv/7Vdxr5yf7x/6wsvzkk4fg+ByBR5cjpfhGNOr/J2LFw332+8wgQKoB27aB+fd9JJMHERAE45+Y55/r6zhHvzIw+LXvzTVZfDjt1HiNHwj//6TtVgtu1CxYv1vCPeJHiO4BE19A2QykoKuBSdwYN8+dz7bUdSU+HSy/1nSxBLVwIe/eqAMQLFUACGtZ2GAVFBVxZ1J0G+Yu5/PJ2pKcbgwf7TpaAggDS0qBzZ99JJAGpABLUFSddQUFRAde4ztQrWMrQoa2oXt0YONB3sgQTBNCpE9So4TuJJKCYOAcgflydeTV/H3AP3w/sQJ2jPuX88x0zZ/pOlUC2bIGVKzX8I97oCCDBDT9lOPmF+dxYdCoZz69g4MAjeOMNo1s338kSwJw5oa8qAPFEBSDc0PEGCooKuLmoPXWeW0Hfvk0IAqNDB9/J4lwQQJ06cPLJvpNIglIBCAAjO40kvyifW4raU3vSSs48syFz5hgnneQ7WZxyDmbNCl38lZzsO40kKJ0DkJ+M6TKG2/pdw47z21OY9gNnnOFYvdp3qji1YQN88YWGf8QrFYD8wp+7/pk/9ruEnYNPJc/9SHa249NPfaeKQ1r+WWKACkD+x1+6/4U/9D+X3CEd2b57J9nZjs8/950qzgQBNGsGLVr4TiIJTAUg/8PMuDP7Tm7q34e8IZ35blse2dmOzZt9J4sThYWhGUA9eoCZ7zSSwFQAUiwz496e9zJiQHf2DO7Gpm/20KOH47vvfCeLAytXwg8/aPhHvFMBSInMjAd7PcjvBrZn7/k9WfdZPj17On74wXeyKm7/+L/u/yueqQCkVGbG38/6O1eefRwFv+nD6jWFnHkmbN/uO1kVFgTQujUcdpjvJJLgVAByUEmWxL/6/otLz2lG4blns/y9Qvr0Ca1kLOWUlwcLFmj4R2KCCkDKJMmS+He/f3PReXUpOnsIixYXMXAg7N7tO1kVs3gx7Nmj4R+JCSoAKbPkpGSeHvA0Qwan4PoPIwjg3HNDy9lLGQUBpKTA6adHfdfTV2ym011zOHLUa3S6aw7TV2haV6JTAUi5JCclM+HsCZx3wW7oczWvvQYXXggFBb6TVRFBAB06QO3aUd3t9BWbGT1tFZtz8nDA5pw8Rk9bpRJIcCoAKbeUpBQmDprIoIu3QK8bePFFuOwyKCrynSzGbdsGy5d7Gf+/d+Za8vILf/FYXn4h985cG/UsEjtUAFIhqcmpPHfOc/S7ZD10/yPPPAPXXBNa40xKMHdu6BfkoQC+yskr1+OSGFQAUmFpyWm8cN4L9L58BXT+G+PGwY03qgRKFARQqxacckrUd904I71cj0tiUAFIpVRLqca086fS86r5cOrDPPQQ/OlPvlPFqCCAbt0gNTXqux7ZqyXpqb9cdjo9NZmRvVpGPYvEDt0PQCqtekp1Xh48nb5F/ZhTUIM77riSGjVgzBjfyWLIF1/Ap5/Ctdd62f3Adk2A0LmAr3LyaJyRzsheLX96XBKTCkDCIj01nRkXvMJZRf2Y/1A6t9wylBo14PrrfSeLEbNnh756vABsYLsmesOXX9AQkIRNjdQavHrhy3S67gloNZUbboBx43ynihFBAI0aQatWvpOI/EQFIGFVK60Wb1w0g1N//zDW4nWuvtrxzDO+U3lWVBQqAC3/LDFGQ0ASdrWr1WbmJTPoUdSX5fdWZ9iw7lSvbpx3nu9knnz4IWzZEtPr/0xfsVnnBxKQCkAiok71Osy6dAbd8/vx/v3VGHJBR9LTk+jb13cyD2J8+ef9Vwnvv1Bs/1XCgEogzmkISCImo3oGs698mRNuGEXRYSsYdE7hT++FCSUI4NhjoWlT30mKpauEE5cKQCLq0PRDmXPVSxx7/QgK6n5Ev/6FLFzoO1UU7d0L8+fH9PCPrhJOXCoAibj6Neoz75pptLj+WvbUXEev3gUsXeo7VZQsWQK5uTFdAGW9SliricYfFYBERcOaDZk/fApHjriSvNSNZPfM54MPfKeKgiCApKTQFcAxqixXCWs10fikApCoaVSrEQtGTOaI31/OTr6lW1Y+H3/sO1WEBUFo7Z86dXwnKdHAdk24c1BrmmSkY0CTjHTuHNT6FyeAdZ4gPmkWkERV49qNWXDjBDoWXMLmhydxeve6/HdhGkcf7TtZBPz4I7z7Lowe7TvJQR3sKmGdJ4hP3o4AzKyZmc01szVmttrMRvjKItHV9JCmLBr5NL+69mK2bt9Bl257+PJL36kiYP58KCyM6fH/stJqovHJ5xBQAXCTc+44oANwrZnpOvkEcXidw1k06nEOu/pivtm6my7d9vD1175ThVkQQI0aoTuAhVm0T8hqNdH45G0IyDn3NfD1vv+9w8zWAE2Aj3xlkuhqntGchbc8Qof8i/jy8efo0t2xZGF16tf3nSxMgiB0799q1cK62XBcuFXeK3+1mmh8iomTwGbWHGgHvFPMc1eZ2TIzW7Zly5aoZ5PIOvrQo1k89n4OvewS1q93nJ61m5wc36nCYPNmWLMmIsM/lT0hW9EZPQPbNWHRqCwePL8tADc8v1LTQas47yeBzawWMBW43jm3/cDnnXPjgHEAmZmZutdUHGpRrwWL/no7HfMvY81/xtO9Zx5vz0mP9n3TwyuCyz+X9YTs/k/5m3PySDaj0DmaZKSza09BiQVysE/0WjYivng9AjCzVEJv/hOdc9N8ZhG/jq1/LIvu/BOHXHgVK99LpedZeeTm+k5VCUEA9etD69Zh33RJJ14d/PSJ/Oef8gEK992nc3NOHjl5+cX+fFlm9Gg6aHzxOQvIgCeBNc65B3zlkNjRqkErFt5zE7XO/x3vLKpG73657NnjO1UFOBcqgOzs0EVgYTR9xWZy9xaU+Pz+T+S3zVj9P2/UB1OWGT2aDhpffB4BdAIuArLMbOW+f2d5zCMxoPVhrVnwwO+oMWgEb8+pQb9BueQX/4E1dq1ZA19/Hfbhn/2f6rfllv4LycsvPOhrDlTWGT2aDhpfvBWAc26hc86cc22cc233/XvdVx6JHW0bteXtR4ZRvd9IZr1eg3OH5FJYvg+zfkVo/L+44ZeKqlsjtdQrf0ui6aDxxftJYJHinNz4ZOY/Vsjp+X/mlal/4cJhuUwaXyPcIyqREQRw9NHQvHlYN1ueYZaM9FT2FBQVWxjpqcmM7Xd8hU7aajpofFEBSMw6pckpzB1XSNdhf+P5Z8dQo8YunvxXzdi+q2JBAcydCxdcEPZNN85I/+mk7s8ZoRPA+6WnJnNr/+MBip0FVNk3bN1cPn6oACSmdWzWkdlPFZI99CGeHnc96em7+MeDMVwCS5fCjh0Rmf45slfLX0zBhNCb/TknN2Hux1uK/USuN2opjQpAYl6XIzrz1oRCegx5nH8+/Ftq1szlnjtq+I5VvCAI3fi9e/ewb1rDLxJu5lzVubYqMzPTLVu2zHcM8WTWutn0/s1mCldczNjbc7n1lhgsga5dYdcu0N+pxBAzW+6cyzzw8apwSk0EgJ6/zmbG5EYknTCF2/5Yg3seiLErxXbuhP/+Ny5W/5TEoAKQKqX3MWfw0vO1sWNf4Q831eAfj8dQCSxYAPn5KgCpMlQAUuX0b9WbF6ckY7+eyXXXVOepCTFyFWoQhFb+7NTJdxKRMlEBSJU0qHUfJj2/B45YwOWXpjLphRgogSCAzp0hXVfFStWgApAqa/BJ/Rn//A/QeBlDh6QwfcZuf2G+/RY++EDDP1KlqACkSrv4lLP59+RNuIarOOccY2bgafW4OXNCX1UAUoWoAKTKu6LTuTw6aR1FGevo07eIeQs8lEAQQN260K5d9PctUkEqAIkLv+v2Gx54dhWFNTfRs1cBS97dG72dOwezZkFWFiQnH/z1IjFCBSBx44YzBnPnhKUUpG3h9OzdrHg/SutIr1sHGzdq+EeqHBWAxJVRfS7gtqcXkW876Nh1J6vXRKEEgiD0VQUgVYwKQOLOn8++kDHj5rInP5/2XX5k3YaS76AVFkEARxwRWgJapApRAUhcumPwUG567E3ydiXRtuMPfLkxQneUKSwMzQDq0YPYXaJUpHgqAIlb9118McMfeZVdOdVp3fFbvvo6AiXw3nuQk6PhH6mSVAAS1/5+5cVccf9LbP+uDid0/Iqt3xeFdwf7x/+zssK7XZEoUAFI3Pv38Eu46M6pbNvUgOM6fMm2nDCWQBBAmzbQsGH4tikSJSoASQjjb7yI39z2Als3NKFVpw3s3BmG+2Dk5sLChRr+kSpLBSAJwcyYPGYo/cc8zzdrjuS4Lp+Ql1fJEli0CPbuVQFIlaUCkIRhZkz/y4X0umkym1a24Phua9izpxIlEASQmgpduoQvpEgUqQAkoZgZb9xzAd2HT+Gzd1vRpseH5OdXsASCADp2hFq1whtSJEpUAJJwzIzZj5zPaVe+wCcLW3PSWe9TWFjOEvj+e1ixQsM/UqWpACQhmRkLHj+HzKEv8WHQllMGvIcrTwfMnRtaBE4FIFWYCkASVpIl8c6EAbQ5dwbvvXYynX7zbtlLIAigdm1o3z6iGUUiSQUgCS3Jklg++SyO7fsm/33xFLIv+W/ZfjAIoHt3SEmJbECRCFIBSMJLSU5m1fSeHN0zYO4zHTnrt4tK/4HPPoP16zX8I1WeCkCEUAl89Ho3Dj99Pm+M68Sg6xeU/OLZs0NfVQBSxakARPZJS0nh47dOo/Gpi3np4S4MGTW/+BcGATRuDMceG92AImGmAhD5mfRqqaydm0nDdkuZfHdXLrtt3i9fUFQUOgLQ8s8SBw5aAGY23MzqRiOMSCyolZ7GJ2+3od4JK3j6ti787p65///JDz6ArVs1/CNxoSxHAI2ApWY2xczONAvfx55921trZuvMbFS4titSWXVqVWPtwuPIaPERj43uzI2P7CuB/cs/Z2f7CycSJgctAOfcH4EWwJPAMOBTM/ubmVXq/ndmlgw8CvQGWgFDzKxVZbYpEk716lTn48W/pvYR63nwxtMYM25uqABatQqdAxCp4sp0DsA554Bv9v0rAOoCL5rZPZXY9ynAOufcBufcXmAyMKAS2xMJu8PqpbN6cTNqNt7IncNP5bb1+Rr+kbhRlnMAvzez5cA9wCKgtXPuGuBk4JxK7LsJsPFn32/a99iB+7/KzJaZ2bItW7ZUYnciFdOsUU1WLf4V6XW/4dYvp3NP3Za+I4mERVmOAOoDg5xzvZxzLzjn8gGcc0VA30rsu7hzCf9zIb5zbpxzLtM5l9mgQYNK7E6k4o5sWpMV50+kWrXv+MPdQ3h4eglTREWqkLKcA/izc+6LEp5bU4l9bwKa/ez7psBXldieSES1fPc13j3uJtLS93D9ha14/I23fUcSqRSf1wEsBVqY2ZFmlgYMBl7xmEekZDk5sHQpbc48kUXz00lJMa4+/9c8PbuUK4ZFYpy3AnDOFQDDgZnAGmCKc261rzwipZo3L3QRWI8eZLauw/w5qSQXpXPZOUcwaeFC3+lEKsTrlcDOudedc8c45452zt3hM4tIqYIAataEU08F4LST6zBzJiTtzWDowEZMe7eMq4iKxBAtBSFSFkEAXbtCWtpPD2V3qssrrxbAjsac2y+DGSuWeAwoUn4qAJGD2bgR1q4t9urfPlmH8uL03bDtSAb2SWfmqnc9BBSpGBWAyMEcZPnnQb0P5dnnd1H03XH06WPMXbssiuFEKk4FIHIwQQANG8IJJ5T4kgvOrscTz2yncHM7evbew6L170UxoEjFqABESuNcqACysyGp9P9cLh9Sn0fGbaPw8450653Du1+8H6WQIhWjAhApzerV8O23ZV7/57rLG3D3I99T8GkWnc/6ivc2rYpwQJGKUwGIlKYCt3+8eXgDxt69hfyPenNa33Ws+uajCIUTqRwVgEhpggBatIDDDy/Xj916cwNGjt3CnvfP5pR+H/DRdx9HKKBIxakAREqSnx+6AriCyz/fc2sDht+8hd3LBtN+4BI+2fppePOJVJIKQKQk774LO3dWav3/R+5qwGXDt5L732GcfO5s1n2/PowBRSpHBSBSkiAI3fi9e/cKb8IMnnikPoMv28rO+Vdz8gUz+Dzn8/BlFKkEFYBISYIAMjOhbt1KbcYMJv67PgMGf8/2t67npIsm8+WPX4YppEjFqQBEirNjByxZErbbPyYlwdRn69Fr4Pdse3UUJ1/+NJu2bwrLtkUqSgUgUpy334aCgrDe/zc5GWZMqUfXXj+wdepYTr76n3y1Q/dAEn9UACLFCQKoXh1OOy2sm01NhZkvH0qHbtv4bvLtZF73AN/u/Das+xApKxWASHGCALp0CZVAmFWrBrNfq0u7U7fz9TN3kXnjHXy367uw70fkYFQAIgf65hv48MOwDv8cqEYNmP9WBse33cWmp+7j1D/cytbcrRHbn0hxVAAiB6rA8g8VUbs2LJxdhxbH7ubzx++nwx9H80PeDxHdp8jPqQBEDhQEcOih0LZtxHeVkQGL5x3CEUcWsP7RBznt1v8jZ3dOxPcrAioAkV8qx/LP4VK/Pix5uzaNG8Pahx+k8x0j+HH3j1HZtyS2FN8BJH5NX7GZe2eu5aucPBpnpDOyV0sGtmtS6ddG1CefwKZNER/+OVCjRrDk7Vpkdshl9f0PcHrKtSwc9Ri1q9WOag5JLDoCkIiYvmIzo6etYnNOHg7YnJPH6GmrmL5ic6VeG3FBEPoa5QIAaNYM/rugBofWrsEH991H9/uvZtfeXVHPIYlDBSARce/MteTlF/7isbz8Qu6dubZSr424IIAjj4Sjjor+vgntdtH8dA5Jy2D53XeT/cgV5Obneski8U8FIGE3fcVmNufkFfvcV8U8XtxjpT0eMQUFMHeul0//P3fssbBgbnVq0oB3/nYHZzx6GXn5Uf5dSEJQAUhY7R/OKUnjjPQyPVba4xGzfDn8+GPoBLBnbdrA3KAa1fc2ZdHtt3HWuEvZXbDbdyyJMyoACavihnP2S09NZmSvlv/z+MheLUlPTS7TayNq//h/VlZ091uC9u1h1ptppO06inl/GUP/Jy9lT8Ee37EkjqgAJKxKG7a5c1DrYmf2DGzXhDsHtaZJRjoGNMlIL/G1ERUEobn/DRpEd7+l6NwZXn81lZScVsy67SbOnjCMvYV7fceSOKFpoBJWjTPSix3/b5KRXuob+sB2TfxM+9xv1y5YvBh+/3t/GUqQnQ3Tp6UwYGA73rh1OOelXMqLF/6H1ORU39GkitMRgIRVzAznlNfChbB3r/cTwCXp0ween5yMbe7IK7dexuDJl1JQVOA7llRxKgAJq5gZzimvIIC0tNCYS4w65xyYMD4J+zyLabcNYegLl1FYVPz5FpGy0BCQhJ334ZyKCILQ2v81a/pOUqqhQyE31/jtb/vw/G27SU25gv+c/QTJSckH/2GRA+gIQGTrVli5MmaHfw501VXw0EPAmnN49rZsrpj+W4pcke9YUgXpCEBiUlTXBpozJ/S1ihQAwIgRkJsLY8YM5T935JGacg3/6vcYSabPdFJ2KgDxqrg3eoDR01b9dD3B/rWBgMiUQBBAnTpw8snh33YEjR4NO3c6/va3K/n333JJSbqOR/v8AzPzHU2qCC8fF8zsXjP72Mw+MLOXzCzDRw7xq6RF4G59ZXV01wYKAujeHVKq3ueh2283rr/ewTsjeOzuplz/5g0453zHkirC1/HiLOAE51wb4BNgtKcc4lFJi8Dl5OUX+/qIrA20YQN89lmVGv75OTN44AHjt791sHA0j9xbi5GzRqoEpEy8FIBz7i3n3P5JzEuApj5yiF/lfUOPyNpAHpd/Dhcz+Oc/jYsvdjD3du6/v4gxs8eoBOSgYuGM0WXAGyU9aWZXmdkyM1u2ZcuWKMaSSCvpDb1ujdToXUwWBNC0KRxzTPi3HUVJSfDkk8Z55zl46wHueiiHsfPG+o4lMS5iBWBmgZl9WMy/AT97zS1AATCxpO0458Y55zKdc5kNYmiNFqm8kq4aHtvv+OhcTFZUFLoBfI8eoY/RVVxKCjz7rNGnr4PXHuOvj3zBX+b/xXcsiWERO+vlnCv1mNrMLgH6AtlOx6oJaf8beknTPSN+MdnKlfDDD1V6+OdAaWnw4gtG336OOa88zdiUwaQm3cnoLjrNJv/Ly7QHMzsT+APQ1Tmn2x0lqEjM9S/XNveP/8fA+v/hVL06vDzd6HUmLH5pEmNSBpGSdC8jO430HU1ijK95b/8AqgGz9s1ZXuKcu9pTFvFg/xTQcM71L/c2gwBOOCF0R/Y4U7MmvP6akd0jmfdenMrNKX1ISUrhho43+I4mMcTXLKBfO+eaOefa7vunN/8EE4n7AJdrm7t3w4IFcTX8c6BDDoGZbxonHJ9C8pQZ3DjuJf7x7j98x5IYUvWufJG4EIn7AJdrm4sXh0ogjgsA4NBDYdZbxuld01g3eSbXpXQjJSmFqzP1mUtiYxqoJKBI3Ae4XNsMgtC0mdNPr/D+qoqGDWHObOOIJtVJnTSba574F0+894TvWBIDVADiRSRuHFOubQYBdOgAtWtXeH9VSePGoRI4rF5NUifN58onH2T8yvG+Y4lnGgKSqPr5LJ2MGqlUS0nix7z8sMwCOti00p9s2wbLlsHYxLpQ6ogjQiXQ5fRD2DbpbYYldyD5imSGthnqO5p4ogKQqDlwls623HzSU5N58Py2YZvzX6ab0cydC87F3fTPsmjRAmYHRteuh7Jz0kIuTulAymUpDD5hsO9o4oGGgCRqSpqlc9OU9zly1Gt0umsO01dsjnyQIIBateDUUyO/rxh0/PEwa5ZRvaAh1SYu4ML/jOTFj170HUs8UAFI1JQ0S6fQuV8sBx3xEggC6NoVUlMju58Y1q4dvPmmkZLbhGqT3mbwhOt4+eOXfceSKFMBSNSUZYZPRNf9B/jiC/j007if/lkWHTrAq68a5DSn2sT5nDvhSl795FXfsSSKVAASNcXN0inO5py8yB0FzJ4d+qoCAEIHQi+9ZBR814Jqz81h0IRhvLnuTd+xJEpUABI1A9s1+cUqn8mlrMAZsaGgIIDDDgsNhAsAvXrBlCnG7o3Hk/b8WwyYMIRZ62f5jiVRoAKQqBrYrgmLRmXx2V19uP83J5Z4RBCxoaC2beHqq+Ni+edwGjAAJk408ja0I+3F1+n3zHnM+WyO71gSYZoGKt7sn655/fMri30+IreAvPnm8G8zTpx/PuTlGZde2pHa016ib/JA3rx4BqcfEf9XSycqHQGIVwPbNaFJBJaFkIoZNgwefRR2rOpOyvTJ9H6mL4u+XOQ7lkSICkC8i8SyEFJxv/sd3Hcf7HivNykz/sOZz5zFkk1LfMeSCNAQkHhX5iUcJGpuugl27YKxYwdxSGoeZyT1YvbFAe2btPcdTcJIBSAxoUxLOEhU/elPkJsLd999IYek5tHTzmDOJbM56Vcn+Y4mYaICEJFimcGdd4ZK4O9/v4I6abvomdSTORfP4cRGJ/qOJ2GgAhCREpnBQw+FSuDJJ0dwSGouPawHcy+ZywkNT/AdTypJBSAipUpKgscfh7w8mDRpNIek5pJt2cy7ZB7HNTjOdzypBBWAiBxUcjKMHx8qgZde+iuk5pJlWcy7ZB4t62u2VlWlaaAiUiYpKfDcc9C7N+x48T5yl59N1oQs1v2wznc0qSAVgIiUWbVqMHUqdOtm7JryKDtW9qT7+O5s2LbBdzSpABWAiJRLejq88gqccoqRN/kpcladRvfx3fk853Pf0aScVAAiUm61asEbb0Cb1knsnfQc369uS9b4LDb+uNF3NCkHFYCIVEidOvDWW3BMiyQKJ07j2zW/pvv47mzeHoXbekpYqABEpMLq1YNZs+DwZsnYpNf5em0TsiZk8fWOr31HkzJQAYhIpTRqFLrRWoN6KaRMCtj4SQZZE7L4due3vqPJQagARKTSmjYNlUDtmqlUf24Bn6+rRvaEbLbs2uI7mpRCBSAiYXHUUaESSE1Ko9bkJXy6voAez/Tg+9zvfUeTEqgARCRsWrYMnRMo2ludulNW8PH6nfR8pifb8rb5jibFUAGISFi1aQMzZ0Le9nQaTP2ADzds5YxnzyBnd47vaHIAFYCIhF1mJrz+Omz7tiaNp3/Iyg0bOfPZM9m+Z7vvaPIzKgARiYhOnWDGDPjmi0M4/JWPWbZhHb0n9mbHnh2+o8k+XgvAzP7PzJyZ1feZQ0QiIysLpk2DjZ9mcPQba1my/kP6TOrDrr27fEcTPBaAmTUDegJf+sogIpF31lmhVUTXr6rHccEnLNywnH7P9SM3P9d3tITn8wjgQeBmwHnMICJRcM45ofsJfLT0MFrP/ZS56xYzYPIA8vLzfEdLaF4KwMz6A5udc+/72L+IRN+FF4buLPbBosacvHgtwafzGDRlELsLdvuOlrAidkcwMwuARsU8dQswBjijjNu5CrgK4PDDDw9bPhGJviuvDN1VbMSIIzg1bQ1vWkvOnXIu086fRlpymu94Cceci+4IjJm1BmYD+wcAmwJfAac4574p7WczMzPdsmXLIpxQRCLtrrtg9GjoNGANi048ngHH9eeF814gNTnVd7S4ZGbLnXOZBz4e9XsCO+dWAQ33f29mnwOZzrmt0c4iIn6MGgW5ufDXvx5H12oredlOZMjUITx3znMqgSjSdQAi4sVtt8GNN8L8KW3I2vAuUz+aykUvXURBUYHvaAkj6kcAB3LONfedQUSizwzuuy90JPCvf7XnjCsW8Lx1ISUphfEDx5OclOw7YtzzXgAikrjM4NFHQyeGxz/Rmd7XBEykBylJKTw14CmSTIMUkaQCEBGvkpLgiSdCJTDlsWz6XP8a49/vQ0pSCuP6jVMJRJB+syLiXUoKPPss9O8Prz10Fv1zp/Lkiid5aMlDvqPFNRWAiMSE1FR4/nno2RNeve9srkwLuPKkK33HimsqABGJGdWrw/Tp0Lmz8dSfspk7s7bvSHFNBSAiMaVGDXj1VejVC5o08Z0mvukksIjEnNq14bXXfKeIfzoCEBFJUCoAEZEEpQIQEUlQKgARkQSlAhARSVAqABGRBKUCEBFJUCoAEZEEFfVbQlaGmW0BvvCdowT1gapyVzNlDb+qkhOUNRJiPecRzrkGBz5YpQoglpnZsuLuuRmLlDX8qkpOUNZIqCo5D6QhIBGRBKUCEBFJUCqA8BnnO0A5KGv4VZWcoKyRUFVy/oLOAYiIJCgdAYiIJCgVgIhIglIBhJmZXWdma81stZnd4zvPwZjZ/5mZM7P6vrMUx8zuNbOPzewDM3vJzDJ8ZzqQmZ257//zdWY2yneekphZMzOba2Zr9v19jvCdqTRmlmxmK8zsVd9ZSmNmGWb24r6/0zVm1tF3prJSAYSRmXUHBgBtnHPHA/d5jlQqM2sG9AS+9J2lFLOAE5xzbYBPgNGe8/yCmSUDjwK9gVbAEDNr5TdViQqAm5xzxwEdgGtjOCvACGCN7xBl8DDwpnPuWOBEqkZmQAUQbtcAdznn9gA4577znOdgHgRuBmJ2JoBz7i3nXMG+b5cATX3mKcYpwDrn3Abn3F5gMqEPATHHOfe1c+69ff97B6E3qpi8666ZNQX6AE/4zlIaMzsEOB14EsA5t9c5l+M1VDmoAMLrGKCLmb1jZvPNrL3vQCUxs/7AZufc+76zlMNlwBu+QxygCbDxZ99vIkbfVH/OzJoD7YB3PEcpyUOEPpwUec5xMEcBW4Cn9w1XPWFmNX2HKivdFL6czCwAGhXz1C2Efp91CR1etwemmNlRztNc24NkHQOcEd1ExSstp3Pu5X2vuYXQEMbEaGYrAyvmsZg9ogIws1rAVOB659x233kOZGZ9ge+cc8vNrJvnOAeTApwEXOece8fMHgZGAX/yG6tsVADl5JzrUdJzZnYNMG3fG/67ZlZEaJGoLdHK93MlZTWz1sCRwPtmBqFhlffM7BTn3DdRjAiU/jsFMLNLgL5Atq8yLcUmoNnPvm8KfOUpy0GZWSqhN/+JzrlpvvOUoBPQ38zOAqoDh5jZs865oZ5zFWcTsMk5t/9I6kVCBVAlaAgovKYDWQBmdgyQRgyuEOicW+Wca+ica+6ca07oj/gkH2/+B2NmZwJ/APo753J95ynGUqCFmR1pZmnAYOAVz5mKZaG2fxJY45x7wHeekjjnRjvnmu772xwMzInRN3/2/Tez0cxa7nsoG/jIY6Ry0RFAeD0FPGVmHwJ7gUti8BNrVfMPoBowa9/RyhLn3NV+I/1/zrkCMxsOzASSgaecc6s9xypJJ+AiYJWZrdz32Bjn3Ov+IsWF64CJ+z4AbAAu9ZynzLQUhIhIgtIQkIhIglIBiIgkKBWAiEiCUgGIiCQoFYCISIJSAYiIJCgVgIhIglIBiFSCmbXfd6+C6mZWc986+yf4ziVSFroQTKSSzOx2QmvWpBNaF+ZOz5FEykQFIFJJ+5YAWArsBk5zzhV6jiRSJhoCEqm8Q4FaQG1CRwIiVYKOAEQqycxeIXQnsCOBXznnhnuOJFImWg1UpBLM7GKgwDk3ad/9gRebWZZzbo7vbCIHoyMAEZEEpXMAIiIJSgUgIpKgVAAiIglKBSAikqBUACIiCUoFICKSoFQAIiIJ6v8BixmVtFPREXYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(np.sum(v1**2),np.sum(v2**2))\n",
    "\n",
    "def linear(x,v,b):\n",
    "    return (b-x*v[0])/v[1]\n",
    "\n",
    "point = np.array([-1,1])\n",
    "points=np.array([-4,4])\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1])\n",
    "plt.plot(point, linear(point,v0,b0),color='red')\n",
    "plt.plot(points, linear(points,v1,b1),color='green')\n",
    "plt.plot(points, linear(points,v2,b2),color='blue')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "The two results are similar. And the result from primal-dual algorithm is a little better than the first one. And it didn't throw out value warnings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "Recall the BCW dataset from HW07. We will use kernel SVMs to train classifiers for this dataset. Instead of coding this up, use the *sklearn.svm* package to fit a kernel SVM using an *rbf* or **Radial Basis Function** kernel. \n",
    "\n",
    "### Part A\n",
    "\n",
    "The Python package *sklearn.svm* is powered by [LIBSVM](http://www.csie.ntu.edu.tw/~cjlin/libsvm). Follow this link and read the documentation. Summarize the optimization program that is LIBSVM is using, and explain the Sequential Minimal Optimization numerical routine that LIBSVM uses. Why do you think they use this approach instead of the log-barrier method or the primal-dual algorithm?\n",
    "\n",
    "### Part B\n",
    "\n",
    "Use the same experimental setup as in HW07 to randomly split and test data, training a kernel SVM with an rbf kernel instead of logisitic regression for each random split of the data. Compare and contrast the ROC curves for this approach versus the ROC curves you obtained in HW07.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution to Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A\n",
    "#### (1) Adjustments to the Orignal Program\n",
    "What they want to solve for the original problem is:\n",
    "$$\\min_{w,b,\\xi}\\frac{1}{2}w^Tw+C\\sum_{i=1}^{l}\\xi_i$$\n",
    "$$subject\\ to\\ y_i(w^T\\phi(x_i)+b)\\geq 1-\\xi_i$$\n",
    "$$\\xi_i \\geq0$$\n",
    "The variables $\\xi_i$ are introduced for the regard that:  \n",
    "  \n",
    "The original program is under the assumption that the given dataset could be somhow separated by some line. But in practical examples, many datasets just cannot be separated by any line (hyper-line or hyper-plane). That may be due to:  \n",
    "  \n",
    "**(A)** errors (systematic errors or errors caused by inaccuracy of measurement);  \n",
    "**(B)** data with labels just doesn't form to the two sides of any line. (For example, data with two labels form in the inner space and outer space of a unit circle)  \n",
    "  \n",
    "If the given dataset cannot be separated by any line, then the feasible set of the original version of SVM program is empty. Thus we couldn't find any solution to the orinal program.  \n",
    "  \n",
    "So $\\xi_i$s are introduced to guarantee that errors of line-separating could happen. And we add a new penalty function to the objective function to ensure that $\\xi_i$ wouldn't be too large. By selecting an appropriate cofficient of $C$, we can make sure that $\\xi$ is just reasonable for the program.\n",
    "\n",
    "In short summary, $\\xi_i$s solve the problems that program couldn't process normally and **(A)** the influence of errors.\n",
    "\n",
    "#### (2) Kernal Functions and the Dual Program of SVM:\n",
    "Although the program could process normally after $\\xi$s are introduced, if **(B)** data is born not to be classified by hyper-planes, result of the program won't be good. The repaired program becomes meanless in those situations.\n",
    "  \n",
    "It's OK to solve the best circle when data is born to be classified by a circle, and solve the best hyperbolic curve when the data is born to be classified by a hyperbolic.\n",
    "  \n",
    "But that requires observations of the original dataset. And this method is not automatic.  \n",
    "  \n",
    "An equivalent (mathematically proved) method is to use a mapping $\\phi(x)$ to map the original dataset into a high-dimension space and then solve the best hyper-plane that separates the data.\n",
    "  \n",
    "Then what we have for the program is: (As mentioned in the beginning)\n",
    "$$\\min_{w,b,\\xi}\\frac{1}{2}w^Tw+C\\sum_{i=1}^{l}\\xi_i$$\n",
    "$$subject\\ to\\ y_i(w^T\\phi(x_i)+b)\\geq 1-\\xi_i$$\n",
    "$$\\xi_i \\geq0$$\n",
    "\n",
    "And the dual program of the above one turns out to be:\n",
    "$$\\min_{\\alpha}\\frac{1}{2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_i\\alpha_jy_iy_j(<\\phi(x_i),\\phi(x_j)>)-\\sum_{i=1}^{N}\\alpha_i$$\n",
    "$$s.t.\\ \\sum_{i=1}^{N}\\alpha_iy_i=0$$\n",
    "$$\\alpha_i\\geq0,\\ i=1,2,\\cdots,N$$\n",
    "\n",
    "The definition of kernal function is: $K(x,y)=<\\phi(x),\\phi(y)>$, the inner product of the two points in the high-dimension space formed by the mapping, then $K(x_i,x_j)=<\\phi(x_i),\\phi(x_j)>$.\n",
    "  \n",
    "We notice that in the program we can focus on $K(x_i,x_j)$ instead of $\\phi(x_i)$. There are two reasons why we focus on kernal function $K(x,y)$ instead of the mapping $\\phi(x)$:  \n",
    "  \n",
    "**(A)** It's faster and more convenient to numerically calculate $K(x_i,x_j)$ than calculate $\\phi(x_i)$ and $\\phi(x_j)$ and finally the innder product of them.  \n",
    "**(B)** It can be mathematically proved that if bivariate function $K(x,y)$ satisfy some conditions, then it could be written as $K(x,y)=<\\phi(x),\\phi(y)>$ with some mapping $\\phi(x)$\n",
    "  \n",
    "So we can focus on finding kernal functions with great features. Plus, some kernal functions are just hard to be written in the inner product form, such as RBF.\n",
    "\n",
    "#### (3) SMO Algorithm\n",
    "SMO algorithm is an algorithm mostly applied to solving the dual program of SVM:\n",
    "$$\\min_{\\alpha}\\frac{1}{2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_i\\alpha_jy_iy_jK_{ij}-\\sum_{i=1}^{N}\\alpha_i$$\n",
    "$$s.t.\\ \\sum_{i=1}^{N}\\alpha_iy_i=0$$\n",
    "$$\\alpha_i\\geq0,\\ i=1,2,\\cdots,N$$\n",
    "$$(K_{ij}=K(x_i,x_j))$$\n",
    "The basic idea is that at each step, we select two variables $\\alpha_i$, $\\alpha_j$, fix all the other variables, and compute the minimum with these two free variables only.  \n",
    "  \n",
    "That is a sub-program as:\n",
    "$$\\min_{\\alpha_1,\\alpha_2}W(\\alpha_1,\\alpha_2)=\\frac{1}{2}K_{11}\\alpha_1^2+\\frac{1}{2}K_{22}\\alpha_2^2+y_1y_2K_{12}\\alpha_1\\alpha_2-(\\alpha_1+\\alpha_2)+y_1\\alpha_1\\sum_{i=3}^{N}y_i\\alpha_iK_{i1}+y_2\\alpha_2\\sum_{i=3}^{N}y_i\\alpha_iK_{i2}$$\n",
    "$$s.t.\\ \\alpha_1y_1+\\alpha_2y_2=-\\sum_{i=3}^{N}y_i\\alpha_i=\\varsigma$$\n",
    "$$0\\leq\\alpha_i\\leq C\\ \\ \\ i=1,2$$\n",
    "That's a easy but complex quadratic program. We can skip all the reduction process and look at the result:  \n",
    "  \n",
    "If we mark:\n",
    "$$E_i=(\\sum_{j=1}^{N}\\alpha_jy_jK(x_j,x_i)+b)-y_i\\ \\ \\ i=1,2$$\n",
    "$$\\eta=K_{11}+K_{22}-2K_{12}=\\Vert\\phi(x_1)-\\phi(x_2)\\Vert^2$$\n",
    "$$\\alpha_2^{new,unc}=\\alpha_2^{old}+\\frac{y_2(E_1-E_2)}{\\eta}$$\n",
    "$$H=\\begin{cases}\n",
    "min(C,C+\\alpha_2^{old}-\\alpha_1^{old})&when\\ y_1\\neq y_2\\\\\n",
    "min(C,\\alpha_2^{old}+\\alpha_1^{old})&when\\ y_1=y_2\n",
    "\\end{cases}$$\n",
    "$$L=\\begin{cases}\n",
    "max(0,\\alpha_2^{old}-\\alpha_2^{old})&when\\ y_1\\neq y_2\\\\\n",
    "max(0,\\alpha_2^{old}+\\alpha_2^{old}-C)&when\\ y_1=y_2\n",
    "\\end{cases}$$\n",
    "Then we have:\n",
    "$$\\alpha_2^{new}=\\begin{cases}\n",
    "H&when\\ \\alpha_2^{new,unc}>H\\\\\n",
    "\\alpha_2^{new,unc}&when\\ L\\leq\\alpha_2^{new,unc}\\leq H\\\\\n",
    "L&when\\ \\alpha_2^{new,unc}<L\n",
    "\\end{cases}$$\n",
    "$$\\alpha_1^{new}=\\alpha_1^{old}+y_1y_2(\\alpha_2^{old}-\\alpha_2^{new})$$\n",
    "That's the computation process of one step.  \n",
    "$$$$\n",
    "$$$$\n",
    "As for how to select the two free variables at each step, we have the strategy below:  \n",
    "\n",
    "**(A)** Selection of the First Variable:  \n",
    "  \n",
    "Select the variable that violates KKT conditions most. The verification rules of KKT conditions are:\n",
    "$$\\alpha_i=0\\Leftrightarrow y_ig(x_i)\\geq1$$\n",
    "$$0<\\alpha_i<C\\Leftrightarrow y_ig(x_i)=1$$\n",
    "$$\\alpha_i=C\\Leftrightarrow y_ig(x_i)\\leq1$$\n",
    "$$in\\ which\\ \\ \\ g(x_i)=\\sum_{j=1}^{N}\\alpha_jy_jK(x_i,x_j)+b$$\n",
    "**(B)** Selection of the Second Variable:  \n",
    "  \n",
    "Suppose the first variable is $\\alpha_i$, we select the variable $\\alpha_j$ with the biggest value of $\\vert E_i-E_j\\vert$ to accelerate the process.\n",
    "$$$$\n",
    "$$$$  \n",
    "\n",
    "   \n",
    "And before updating $\\alpha_i$ at each step, we need to update $b$ first:\n",
    "$$b_1^{new}=y_1-\\sum_{i=3}^{N}\\alpha_iy_iK_{i1}-\\alpha_1^{new}y_1K_{11}-\\alpha_2^{new}y_2K_{21}$$\n",
    "\n",
    "That's the whole thing for SMO.\n",
    "\n",
    "### Comment\n",
    "#### Why use SMO\n",
    "(1) Methods of log-barrier and primal-dual used in **Problem 1** focus on the original variables or original variables as well as dual variables. So it will be hard to do computations when we map the data into a higher-dimension space with $\\phi(x)$. They cannot fit well with kernal functions.  \n",
    "\n",
    "(2) The two methods both use iteration steps which require a gradient descent direction. But SMO doesn't require that. SMO is the algorithm fit well with the dual program of SVM with efficiency and stability.----Since the dual program of SVM is a program with only one equation restriction and some simple inequation restriction, it is reasonable to use a method that is more specialized designed for the program. Log-barrier and primal-dual are more general methods while SMO is special for SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n"
     ]
    }
   ],
   "source": [
    "### Load the BCW data using scikit-learn\n",
    "import numpy as np\n",
    "import numpy.random as rd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "BCW = load_breast_cancer()\n",
    "print(BCW.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import normalize\n",
    "y=BCW.target\n",
    "one = np.ones((569,1))\n",
    "X=scale(BCW.data)\n",
    "X = np.concatenate((one, X), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from numpy.random import shuffle\n",
    "order=np.arange(569)\n",
    "shuffle(order)\n",
    "\n",
    "test=[X[order[0:114],], X[order[114:228],], X[order[228:342],], X[order[342:456],], X[order[456:569],]]\n",
    "train=[X[order[114:569],], X[order[np.append(np.arange(114), np.arange(228,569))],], \\\n",
    "       X[order[np.append(np.arange(228), np.arange(342,569))],],\\\n",
    "      X[order[np.append(np.arange(342), np.arange(456,569))],],\\\n",
    "      X[order[:456],]]\n",
    "\n",
    "target_train=[y[order[114:569],], y[order[np.append(np.arange(114), np.arange(228,569))],], \\\n",
    "       y[order[np.append(np.arange(228), np.arange(342,569))],],\\\n",
    "      y[order[np.append(np.arange(342), np.arange(456,569))],],\\\n",
    "      y[order[:456],]]\n",
    "target_test=[y[order[0:114],], y[order[114:228],], y[order[228:342],], y[order[342:456],], y[order[456:569],]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 100% (1/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "Accuracy = 0% (0/1) (classification)\n"
     ]
    }
   ],
   "source": [
    "####SVM\n",
    "from libsvm.svmutil import *\n",
    "svm_model.predict = lambda self, x: svm_predict([0], [x], self)[0][0]\n",
    "\n",
    "predict=[]\n",
    "\n",
    "for i in range(5):\n",
    "    prob = svm_problem(target_train[i],train[i])\n",
    "\n",
    "    param = svm_parameter()\n",
    "    param.kernel_type = RBF\n",
    "    param.C = 2**(1)\n",
    "    param.gamma=2**(-5)\n",
    "    m=svm_train(prob, param)\n",
    "\n",
    "    one_predict=[]\n",
    "    for point in test[i].tolist():\n",
    "        one_predict.append(m.predict(point))\n",
    "    predict.extend(one_predict)\n",
    "\n",
    "predict=np.array(predict)\n",
    "predict.shape\n",
    "fpr, tpr, thersholds = roc_curve(y[order], predict, pos_label=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####logistic regression\n",
    "def logit(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def ell(b,y,X,lam):\n",
    "    return np.sum(np.log(1+np.exp(((-1)**y)*(X@b))))+lam*(np.linalg.norm(b)**2)\n",
    "\n",
    "def Dell(b,y,X,lam):\n",
    "    return X.T @ (((-1)**y)*logit(((-1)**y)*(X@b)))+2*lam*b\n",
    "\n",
    "def D2ell(b,y,X,lam):\n",
    "    return (X.T * (logit(-X@b)*logit(X@b))) @ X+2*lam*np.identity(len(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtracking(x0, dx, f, df0, alpha=0.2, beta=0.8):\n",
    "    '''\n",
    "    Backtracking for general functions with illustrations\n",
    "    :param x0: Previous point from backtracking, or initial guess\n",
    "    :param dx: Incremental factor for updating x0\n",
    "    :param f: Objective function\n",
    "    :param df0: Gradient of f at x0\n",
    "    :param alpha: Sloping factor of stopping criterion\n",
    "    :param beta: \"Agressiveness\" parameter for backtracking steps\n",
    "    :param verbose: Boolean for providing plots and data\n",
    "    :return: x1, the next iterate in backtracking\n",
    "    '''\n",
    "\n",
    "    # Note that the definition below requires that dx and df0 have the same shape\n",
    "    delta = alpha * np.sum(dx * df0) # A general, but memory intensive inner product; df0.T @ dx in Python 3.5+\n",
    "    \n",
    "    t = 1 # Initialize t=beta^0\n",
    "    f0 = f(x0) # Evaluate for future use\n",
    "    x = x0 + dx # Initialize x_{0, inner}\n",
    "    fx = f(x)\n",
    "    \n",
    "    while (not np.isfinite(fx)) or f0 + delta * t < fx:\n",
    "        t = beta * t\n",
    "        x = x0 + t * dx\n",
    "        fx = f(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######\n",
      "Training set 0\n",
      "Newton Steps: 13\n",
      "Predicting set 0\n",
      "#######\n",
      "Training set 1\n",
      "Newton Steps: 12\n",
      "Predicting set 1\n",
      "#######\n",
      "Training set 2\n",
      "Newton Steps: 12\n",
      "Predicting set 2\n",
      "#######\n",
      "Training set 3\n",
      "Newton Steps: 11\n",
      "Predicting set 3\n",
      "#######\n",
      "Training set 4\n",
      "Newton Steps: 13\n",
      "Predicting set 4\n",
      "#######\n",
      "Prediction Complete\n"
     ]
    }
   ],
   "source": [
    "#####logistic regression\n",
    "import matplotlib.pyplot as plt\n",
    "max_steps = 10000\n",
    "lam=0.01\n",
    "stop_limit=1e-6\n",
    "\n",
    "predict=[]\n",
    "\n",
    "for index in range(5):\n",
    "    print('#######')\n",
    "    print('Training set', index)\n",
    "    f = lambda b: ell(b,target_train[index],train[index],lam) # initialize the negative log likelihood as a function of just b\n",
    "    b = rd.randn(31) # Initialize \\beta_0\n",
    "\n",
    "    def nt_4(b):\n",
    "        k2=0\n",
    "        while(k2 < max_steps):\n",
    "            gra=Dell(b,target_train[index],train[index],lam)\n",
    "            v = np.linalg.solve(D2ell(b,target_train[index],train[index],lam),gra)\n",
    "            b = backtracking(b, -v, f, gra)\n",
    "            if(np.linalg.norm(gra) < stop_limit):\n",
    "                break;\n",
    "            k2=k2+1\n",
    "        return k2, b\n",
    "\n",
    "    k, b=nt_4(b)\n",
    "    print('Newton Steps:', k)\n",
    "    print('Predicting set', index)\n",
    "    one_predict=logit(test[index]@b)\n",
    "    predict.extend(one_predict)\n",
    "print('#######')\n",
    "predict=np.array(predict)\n",
    "print('Prediction Complete')\n",
    "\n",
    "#print(predict.shape)\n",
    "fpr1, tpr1, thersholds = roc_curve(y[order], predict, pos_label=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAu2ElEQVR4nO3deZgU1dn38e/NsAz7rlFZVcQNGHVcwBDGJUaMPmqiIiQazUKMu0YjLjGv0bjEDTfUiSEoimgEjSKK0UeQJ6KCZkRcQESUiRgBYWQVBu73j1PDNLM2MN3FTP0+19VXV/U51XVXT0/dfWo5x9wdERFJrkZxByAiIvFSIhARSTglAhGRhFMiEBFJOCUCEZGEUyIQEUk4JQIRkYRTIpAGxcwWmtlaM1tlZl+a2Rgza1WhzgAz+18zW2lmJWb2nJntW6FOGzMbaWafR+81P5rvVM16zcwuNLM5ZrbazIrN7O9m1ieT2ytSF5QIpCE6wd1bAXnAAcCVZQVm1h94CfgHsCvQE3gX+JeZ7R7VaQq8AuwHHAu0AQYAy4BDqlnnXcBFwIVAB2Av4Bngh1sbvJk13tplRLaH6c5iaUjMbCHwS3d/OZr/M7Cfu/8wmp8OvOfu51ZY7gVgibufaWa/BP4E7OHuq9JYZy/gI6C/u79VTZ2pwKPu/lA0f1YU53ejeQfOBy4GGgNTgFXuflnKe/wDmObud5jZrsA9wPeAVcCd7n537Z+QSGVqEUiDZWZdgMHA/Gi+BeGX/d+rqP4k8P1o+mjgxXSSQOQooLi6JLAVTgIOBfYFxgFDzMwAzKw9cAww3swaAc8RWjK7Reu/2Mx+sJ3rl4RSIpCG6BkzWwksAr4C/hC93oHwnV9cxTKLgbLj/x2rqVOdra1fnZvc/Wt3XwtMBxwYGJWdAsxw9y+Ag4HO7v5Hd1/v7guAvwCn10EMkkBKBNIQneTurYECYG/Kd/DLgU3ALlUsswuwNJpeVk2d6mxt/eosKpvwcMx2PDA0emkY8Fg03R3Y1cxWlD2Aq4Cd6yAGSSAlAmmw3H0aMAa4LZpfDcwATq2i+mmEE8QALwM/MLOWaa7qFaCLmeXXUGc10CJl/jtVhVxh/nHgFDPrTjhkNCF6fRHwqbu3S3m0dvfj0oxXZAtKBNLQjQS+b2Z50fwI4GfRpZ6tzay9md0A9Aeui+qMJexsJ5jZ3mbWyMw6mtlVZlZpZ+vuHwOjgMfNrMDMmppZrpmdbmYjompFwI/MrIWZ7Qn8orbA3f3fwBLgIWCKu6+Iit4CvjGzK8ysuZnlmNn+Znbw1n44IqBEIA2cuy8BHgF+H83/H/AD4EeE4/qfES4x/W60Q8fdvyWcMP4I+CfwDWHn2wl4s5pVXQjcC9wHrAA+AU4mnNQFuBNYD/wXeJjywzy1eTyKZVzKNm0ETiBcHvsp4ZDWQ0DbNN9TZAu6fFREJOHUIhARSTglAhGRhFMiEBFJOCUCEZGEq3edW3Xq1Ml79OgRdxgiIvXK22+/vdTdO1dVVu8SQY8ePZg1a1bcYYiI1Ctm9ll1ZTo0JCKScEoEIiIJp0QgIpJwSgQiIgmnRCAiknAZSwRmNtrMvjKzOdWUm5ndHQ0KPtvMDsxULCIiUr1MtgjGEAb+rs5goFf0GA7cn8FYRESkGhm7j8DdXzOzHjVUORF4JBqJ6Q0za2dmu7h7XQz5J9uhsBDGRZ0ed+kCjz4api++GIqKtqy7116hPsDw4TBv3pbleXkwcmSY/ulPobh4y/L+/eGmm8L0j38My5ZtWX7UUfD734fpwYNh7doty48/Hi6LhncvKKi8LaedBueeC2vWwHFVDNty1lnhsXQpnHJK5fLf/AaGDIFFi+CMMyqX//a3cMIJMHcu/PrXlcuvuQaOPjp8bhdfXLn8xhthwAB4/XW46qrK5SNHhs/w5Zfhhhsqlz/4IPTuDc89B7ffXrl87Fjo2hWeeALur+Kn1lNPQadOMGZMeFQ0eTK0aAGjRsGTT1Yunzo1PN92G0yatGVZ8+bwwgth+vrr4ZVXtizv2BEmREPtXHklzJixZXmXLvDoWAf38N17l2joHodGOeG7N3INrFvH8ItbMO+TRuVD+zRtGr57Vy+BVav46cWdKP4yJ5Qb0LxF+O4N/xRKSvjxb7uzbEXj8N7WCFq1Ct+9/3kXli9n8O/6sPbbRqE8JwfatA3fvUNeg+XLKbh6QHlsTZpAu/bhu7frM6z5ahXH3XFUeWzNmkGHDuG7t2k0S78s5ZTRx4VlnfCBd+wYvnsLb2HRV804Y8JJ0KwpU+fuWvmPUAfivKFsN1KG5gOKo9cqJQIzG05oNdCtW7esBJdpqTtb2LH+IW++OUwPGrSVG1UnvPwpjNsO69ZBaSmU5kLp5kJoHH19lyyB9evh244pO4pG0LRZKP/kE1j+LazuVl6e0zh8MABvvw3/3Qglvdn8z9ikCbSMBij75z/hixxY1q+8PLcZtGodyp94Aopbwn8PLi9v2QJat4ENG+DB0fBZe/jP4eXlbdqEx5o1cMs98Oku8Nmg8vIO7aFNW1i+HK66FRbsDguOKI+/c+dQ/sUXcNcNsGA/mHd0efkuu4b3nzcPRlwHCw+Bj1PKu3WH1q3hnXfg7mvh8yOh+Ojy9e+xB7RqBf/7v3DztfDFSbD0yPLyffaGFi3DF/e66+CrM2FlSvz9+obPt7AwZLAVF8C3h5evP/9goGn4FXDTTbDuWth4SHn54YcDjUOmvfNOwnAOeeVfk0GDwnfg4ovhL38BHgT2CmWNcmBgNNTzhRfC+PGEsYa6hNeaNoX+A8rLJ00CniIMPU2I+5BDw/RFF8G0acBkIPq+tGoFB0WD0V1ySfgMebU8trbtIK99mP7d7+Dj4mj5SIeO0KFDmL76avhyQ3nsADvtFP4pIfzDrm4ftr11GyAziSCj4xFELYJJ7r5/FWXPEwbr/r9o/hXgd+7+dk3vmZ+f71m5s/jTT2HTpvAPsWoVvPRSmE99HHww9OoVfk4++2x4zb28/Kijwk/m4mL4+983v144ow+/fjocNRs0CFi7hqe+dw+dclcxpqgfY2YfGP4hdtklfClXrWTyoX+kRc63jHr3cJ6cf0Ao79YNcnNhxQqmHnAJbNrEbXOOZVJxv1C+5x7QtBnNV/6XF7qdA5s2cf1Hp/LK0r7RP/M+0KQJHVd/xoTWZ8OmTVz5yS+Z8c1+DOv0EsNnnx92hiNHwiOPVN7+998PO+trrglZLbWseXP4+OPwWZ5zzhbbz6ZN4cv+ySeh/KSTwk/aTZvKP/899yxf/ogjyjNdmQMOiP4Bgfz8sDNP9b3vRf/AhJ/MFZsqP/xhecbcdVdYXOH3x+mnw+OPh+nWrcN3INWvflXeFCpLWKkuvTT8RF+1Kixf0bXXhh3o4sVh/RX9+c9w+eXhM9hvv5DYUh8jR8LPfw6zZ4cmh9mW5XffDSefDG++GZpiFZe/557QhHrttRBrxfK774YDDwyJ4MYby18vW8/IkeG7/89/wgMPVF7+1lvDdk2ZEn65pJaZhfds1w5efDH8b1Vc/tprw3d7ypSwDRWX/93vwvMrr8CcOVuWN20Kv4gGgJs2DRYu3LK8efPwnYPQDPnvf7csb9UqfH8gfMe++aZyed++ofzDD8MPldTyli2hrBuczz+HjRu3/OyaNy/f0X/9dXhOXb5Jk9BqgPBDIicnLFvV92wrmNnb7l7lcKpxJoIHganu/ng0PxcoqO3QUNYSQb9+8N3vwn33hX/GvfaqXOf+++Gccyi85jPG/elTAB7k1/RmHs9xPLfv/RDsvDOsWAHvFgEwljN4nQHczAh+c24jht/XD55/PhzjSP2yNGoU/kkKCsLP9V/8ovI/ywsvhB3i44/DiBGVyydPDols7NjQVKj4zzxpUtgh/+1v4VFx+aefDl/q0aPhmWcqL//EE+F59Oiwo05dNjcX7r03fE5jxoQddWp5mzbwhz+E8kcfDcdWUre9Qwc4//xQPn58SKapy++0U9hZA0ycGI4ppZbvvDMcG52imjwZVq/esvw734FDo199r74a/uEqvv+++4bymTNDgk/d/k6dwjEXCDuDip9d27ZhGzZtqryjMQs7g+bNw/tW3JGU1RGpQztqIvghcD5wHGFg7rvd/ZDa3jNriWCnnUIimDgRvv027Kgq/LO+/MGu3DCy1eYfnoMOW8eDt5TQu9cmnns5l9v/0jbUdQ+/CgzGPrSert2sPPOXlesfX0QyqKZEkLFzBGb2OFAAdDKzYuAPQBMAd3+AcNDsOGA+sAY4O1OxbJNvvglNXwjNtKgpWFgYWssjRgCtQvGgQTBsGAwfngvkAnDCGeERGOUfdRUfuZKAiMQok1cNDa2l3IHzMrX+7fLtt+HRtvJY4OPGhcOORx4ZrpI5+ugY4hMRqUON4g5gh1RSEp6rSAQQWgDDh2cxHhGRDFIiqEpubrhkrewSNBGRBqzeDUyTFW3aVH33j4hIA6REUJWSknCjzu67Q7Nmm2/+uvHG8BARaUh0aKgqL78criH/6COg/AQxhO4ABgyIMTYRkTqmRFBBYSG8PCN0LVD0n84UFIR+YgYNUgIQkYZJh4YqGDcOGn+xL0dDuJWc0OnXsGFxRiUikjlKBCkKC8MhoEHdQ583eYe3rNTFjYhIQ6NDQynKegMdtudboTWQkxNvQCIiWaAWQQWDBsHwG3vA3FFxhyIikhVKBCkefDCa6H0IHFJr/3ciIg2CEkGK3r2jiaKi0EXw5hdERBouJYIUzz0Xnk/4469Cf/Nlw3qJiDRgSgQpysZ8PaGkJIyQJSKSALpqqColJdX2PCoi0tAoEVRFiUBEEkSJoKJNm6odlEZEpCHSOYKKjDBQu64YEpGEUCJIMXYsQCPoemLcoYiIZI0ODaXo2hW6tlgGzz8PX38ddzgiIlmhRJDiiSfgiTu/gOOPhzlz4g5HRCQrlAgihYVw+ulw/9PfCS/oZLGIJIQSQWRzz6P588KEEoGIJIQSQYpBg2B43lthRolARBJCiaCikpLw3KZNvHGIiGSJLh+NPPVUNLH6bBg4UIPSiEhiKBFEOnUqm+gO3bvHGouISDYpEUTGjAnPZ3V9JXQz8f3vxxqPiEi2KBFENieCxjfBmjVKBCKSGIk/WVxYCAUFYVAyQD2PikjiJD4RjBsH06ZBXh4MG0ZIBO3axRyViEj2ZPTQkJkdC9wF5AAPufvNFcrbAo8C3aJYbnP3v2UypoomTw7PLVpEL/xeLQIRSZaMtQjMLAe4DxgM7AsMNbN9K1Q7D/jA3fsBBcDtZtY0UzFVpUWLlCQAOjQkIomTyRbBIcB8d18AYGbjgROBD1LqONDazAxoBXwNlGYwpkpGjQrP554LuMPMmdC+fTZDEBGJVSbPEewGLEqZL45eS3UvsA/wBfAecJG7b6r4RmY23MxmmdmsJUuW1GmQTz4ZHtGKoE8f6NKlTtchIrIjy2QisCpe8wrzPwCKgF2BPOBeM6vUt4O7F7p7vrvnd+7cua7jLLdsGdx7LyxYkLl1iIjsYDKZCIqBrinzXQi//FOdDUz0YD7wKbB3BmOq2aefwgUXwPvvxxaCiEi2ZTIRzAR6mVnP6ATw6cCzFep8DhwFYGY7A72B+H6Ol3U4p5PFIpIgGTtZ7O6lZnY+MIVw+ehod3/fzM6Jyh8ArgfGmNl7hENJV7j70kzFVCslAhFJoIzeR+Duk4HJFV57IGX6C+CYTMZQm6lTU2aUCEQkgRJ/Z/EWlAhEJIESnwhuuy08APjlL+Hjj5UIRCRREp8IJk0KDwBatYI994RGif9YRCRBtMdLNXEiPPBA7fVERBoQJYJU48bBPffEHYWISFYpEaRasULnB0QkcRI/Qlnz5ikzJSUpgxeLiCRD4hPBCy+kzJSUwB57xBaLiEgcdGgolcYiEJEESnyL4Prrw/Pvf0/odXRTpV6wRUQatLRbBGbWMpOBxOWVV8IDgJYtoXXrWOMREcm2WhOBmQ0wsw+AD6P5fmY2KuORZVtJCVx6Kbz1VtyRiIhkVTotgjsJA8gsA3D3d4HvZTKoWHz1Fdx5J8ydG3ckIiJZldahIXdfVOGljRmIJV7qcE5EEiqdk8WLzGwA4NEAMxcSHSZqCDp2jCbKEkG7dnGFIiISi3QSwTnAXYSB54uBl4BzMxlUNk2YEE1MVItARJIpnUTQ291/kvqCmR0O/CszIcVk9erwrEQgIgmTzjmCqnphazA9s115ZXhwxhlQWgrdu8cdkohIVlXbIjCz/sAAoLOZXZpS1IYwBnGDMGNGykxOg9ksEZG01dQiaAq0IiSL1imPb4BTMh9alj36aLiPQEQkYaptEbj7NGCamY1x98+yGFM8Xn0VpkyBO+6IOxIRkaxK52TxGjO7FdgPyC170d2PzFhUcVCHcyKSUOmcLH4M+AjoCVwHLARmZjCmrOrSJTyUCEQkqdJpEXR097+a2UUph4umZTqwbHn00WjikJKUu8tERJIjnUSwIXpebGY/BL4AumQupJg0bgydO8cdhYhI1qWTCG4ws7bAbwn3D7QBLs5kUNl08cXheeTrr8cah4hIXGpNBO4+KZosAY6AzXcWNwhFRXFHICISr2pPFptZjpkNNbPLzGz/6LXjzex14N6sRZgNvglOPhmeey7uSEREsq6mFsFfga7AW8DdZvYZ0B8Y4e7PZCG27CkthWeegaOOijsSEZGsqykR5AN93X2TmeUCS4E93f3L7ISWRaXR8Aq6fFREEqim+wjWu/smAHdfB8zb2iRgZsea2Vwzm29mI6qpU2BmRWb2fhyXpe61F+y166owo0QgIglUU4tgbzObHU0bsEc0b4C7e9+a3tjMcoD7gO8TxjGYaWbPuvsHKXXaAaOAY939czPbads3ZdsUFgKvzIZnUCIQkUSqKRHss53vfQgw390XAJjZeOBE4IOUOsOAie7+OYC7f7Wd69x23btDhw6xrV5EJC41dTq3vR3N7QakjnVcDBxaoc5eQBMzm0ro2fQud3+k4huZ2XBgOEC3bt22M6wtDR8OcBSFCxfW6fuKiNQX6dxQtq2site8ivUfBBwFNAdmmNkb7j5vi4XcC4FCgPz8/IrvsV3mzau9johIQ5ZOp3Pbqphw+WmZLoTuKSrWedHdV7v7UuA1oF8GY6ral4vh2GPB6zTHiIjUC2klAjNrbma9t/K9ZwK9zKynmTUFTgeerVDnH8BAM2tsZi0Ih44+3Mr1bL/Vq+Ff/wKrqhEjItKw1ZoIzOwEoAh4MZrPM7OKO/RK3L0UOB+YQti5P+nu75vZOWZ2TlTnw+h9ZxNuXHvI3eds47Zsu9JSXTEkIomVzjmC/0e4AmgqgLsXmVmPdN7c3ScDkyu89kCF+VuBW9N5v0zIywP+Mx+aKhGISDKlkwhK3b3EGuhhk5EjgffvgtVKBCKSTOkkgjlmNgzIMbNewIVAw+qzuWtXaNIk7ihERGKRzsniCwjjFX8LjCN0R31xBmPKqp/+FH66fjQ8+GDcoYiIxCKdFkFvd78auDrTwcShuDjuCERE4pVOi+AOM/vIzK43s/0yHlEc3nkbRo2KOwoRkVjUmgjc/QigAFgCFJrZe2Z2TaYDyxrfBCtXwvLlcUciIhKLtG4oc/cv3f1u4BzCPQXXZjKorCotDc+6j0BEEiqdG8r2MbP/Z2ZzCENUvk7oLqJB6L//KvozQ4lARBIrnZPFfwMeB45x94p9BdV7N/1qARReBW3/EXcoIiKxqDURuPth2QgkNk2bQkEB7LZb3JGIiMSi2kRgZk+6+2lm9h5bdh+d1ghl9cWPr+sLHV5lwkFxRyIiEo+aWgQXRc/HZyOQuCxbFncEIiLxqvZksbsvjibPdffPUh/AudkJLwsWL4a33oRvvok7EhGRWKRz+ej3q3htcF0HEptvv4W1a6FFi7gjERGJRU3nCH5D+OW/u5nNTilqDfwr04FlzcZSaJQDjTM5aqeIyI6rpr3fOOAF4CZgRMrrK93964xGlUVH7fQelMwHBsYdiohILGpKBO7uC83svIoFZtahoSSD3/d8DNZ+AFwedygiIrGorUVwPPA24fLR1JFpHNg9g3Flz0EHQbducUchIhKbahOBux8fPffMXjjZN/i1K4FwDExEJInS6WvocDNrGU3/1MzuMLMG8xN67drwEBFJqnQuH70fWGNm/YDfAZ8BYzMaVTa9+SYs+CTuKEREYpNOIih1dwdOBO5y97sIl5A2DN9+G3cEIiKxSufi+ZVmdiVwBjDQzHKAhjHS+/r1YWCaHN1DICLJlU6LYAhh4Pqfu/uXwG7ArRmNKltKSjieSRyfp4GLRSS50hmq8kvgMaCtmR0PrHP3RzIeWTaUlHAZt3PZjxbEHYmISGzSuWroNOAt4FTgNOBNMzsl04FlRW4u/PznsPfecUciIhKbdA6OXw0c7O5fAZhZZ+Bl4KlMBpYVXbpQ8Mlf4XKYOjXuYERE4pHOOYJGZUkgsizN5XZ8mzax5Zg7IiLJk06L4EUzm0IYtxjCyePJmQspix55BKb1hEMPBXLjjkZEJBbpjFl8uZn9CPguob+hQnd/OuORZUNJCeDQOCfuSEREYlPTeAS9gNuAPYD3gMvc/T/ZCiwrSkrCs+4jEJEEq+lY/2hgEvBjQg+k92ztm5vZsWY218zmm9mIGuodbGYbs341UkkJpzX9B6cNsdrriog0UDX9FG7t7n+Jpuea2Ttb88bRHcj3EYa6LAZmmtmz7v5BFfVuAaZszfvXiRUrOLfTi3DunVlftYjIjqKmRJBrZgdQPg5B89R5d68tMRwCzHf3BQBmNp7QX9EHFepdAEwADt7K2Lff0UezZtc9YY2GLBaR5KopESwG7kiZ/zJl3oEja3nv3YBFKfPFwKGpFcxsN+Dk6L2qTQRmNhwYDtCtLgeRGTqU4wqA6bqPQESSq6aBaY7Yzveu6sB7xYv2RwJXuPtGs+qP07t7IVAIkJ+fX2cX/heOXMO0aS0YNKiu3lFEpP7J5OUyxUDXlPkuwBcV6uQD46Mk0Ak4zsxK3f2ZDMa12bir3gMOZdiwbKxNRGTHlMlEMBPoZWY9gf8ApwNb7HJTh8E0szHApGwlAQBKNzJol7kMH947a6sUEdnRZCwRuHupmZ1PuBooBxjt7u+b2TlR+QOZWne6zvK/Qb+jASUCEUmuWhOBheM2PwF2d/c/RuMVf8fd36ptWXefTIXuKKpLAO5+VloR15X16zmr9CE4vHtWVysisqNJp/O4UUB/YGg0v5Jwf0D9VlLCUjqyNGfnuCMREYlVOongUHc/D1gH4O7LgaYZjSobmjXjlB5vc8pTp8cdiYhIrNI5R7AhuvvXYfN4BJsyGlU2tGkD3dvEHYWISOzSaRHcDTwN7GRmfwL+D7gxo1Flw+rV8O26MHi9iEiCpdMN9WNm9jZwFOEmsZPc/cOMR5ZpL70Eb7SHg/KBVnFHIyISm3SuGuoGrAGeS33N3T/PZGAZV1ICtNdYBCKSeOmcI3iecH7ACMN49QTmAvtlMK7MKynhN9wPv8qPOxIRkVilc2ioT+q8mR0I/DpjEWVLSQlDeBJ+9ljckYiIxGqrB6GPup/OfpfRdW3FChbl9mLRYo1OJiLJls45gktTZhsBBwJLMhZRtpx8MmdMugzOUBfUIpJs6fwcbp0yXUo4ZzAhM+Fk0cCBsGvcQYiIxK/GRBDdSNbK3S/PUjzZM3curOsOublxRyIiEqtqzxGYWWN330g4FNTwnHEGzJsXdxQiIrGrqUXwFiEJFJnZs8DfgdVlhe4+McOxZVZJie4hEBEhvXMEHYBlhHGFy+4ncKDeJ4LfHvAqnF+/b4cQEdleNSWCnaIrhuZQngDK1Nm4wbEpKeGEvp/BCXEHIiISr5oSQQ6hE550BqGvX9avh3XrmLu+J8yF3hqgTEQSrKZEsNjd/5i1SLLJDB57jF/f+T/wru4jEJFkq+nO4qpaAg1DkyYwbBi0VK+jIiI1JYKjshZFlhWOXENB3gqK/l2/j3CJiNSFahOBu3+dzUCyafcNc2n87izyepYwbFjc0YiIxCuRPa4dvedCjuZH8Ld34IAD4g5HRCRWW937aENQ9F4ORfSDtm3jDkVEJHaJbBFc/MiBwEimKhGIiCSzRcDG0vDcpk28cYiI7AAS2SJgp53CpaNNmsQdiYhI7JKZCJq3CA8REUloIigpIfSS0S7mQERE4pfIRHBjkz/A6tXAX+IORUQkdolMBANsBnRpF3cYIiI7hIxeNWRmx5rZXDObb2Yjqij/iZnNjh6vm1m/TMZT5vUvd+f1DQdnY1UiIju8jLUIovGO7wO+DxQDM83sWXf/IKXap8Agd19uZoOBQuDQTMUEUFgIv/78cQatn8vUTK5IRKSeyGSL4BBgvrsvcPf1wHjgxNQK7v66uy+PZt8AumQwHgDGjQvPw/rMzvSqRETqhUwmgt2ARSnzxdFr1fkF8EJVBWY23MxmmdmsJUuWbHNARUXhMeiAbxh+/4Hb/D4iIg1JJk8Wpz2ymZkdQUgE362q3N0LCYeNyM/P366+o/PyYNiwNrCH7ioWEYHMtgiKga4p812ALypWMrO+wEPAie6+LFPBvPwyLF0KU59ezvAmf4PPPsvUqkRE6pVMJoKZQC8z62lmTYHTgWdTK5hZN2AicIa7z8tgLNxwQ3iwYAH8/OfhGJGIiGTu0JC7l5rZ+cAUIAcY7e7vm9k5UfkDwLVAR2CUmQGUunt+pmICoruKURfUIiKRjN5Q5u6TgckVXnsgZfqXwC8zGUMlSgQiIltIXjfUSgQiIltQIhARSbjE9DX04IPRxE5nQkEBtGsXYzQiIjuOxCSC3r3LptpD+/ZxhiIiskNJTCJ47rnwfELOZFiyBH72s3gDEhHZQSQmEdx+e3g+ofPfYM4cJQIRkUhiEsFmJSU6USxST23YsIHi4mLWrVsXdyg7rNzcXLp06UKTrRiTPZmJQCeKReql4uJiWrduTY8ePYhuQpUU7s6yZcsoLi6mZ8+eaS+XzMtH1SIQqZfWrVtHx44dlQSqYWZ07Nhxq1tMSgQiUq8oCdRsWz6fxBwaGjs2mmj2LjRKXv4TEalOYvaIXbuGBzvtBJ06xR2OiNRTf/rTn9hvv/3o27cveXl5DB48mCuvvHKLOkVFReyzzz4A9OjRg4EDB25RnpeXx/7775+1mGuTmBbBE08Aa9cyZMFNcNJJcKBGKBORrTNjxgwmTZrEO++8Q7NmzVi6dCnvv/8+Z599NjfddNPmeuPHj2fYsGGb51euXMmiRYvo2rUrH374YRyh1ygxieD++4G1MOSt62HPPZUIRBqCgoLKr512Gpx7LqxZA8cdV7n8rLPCY+lSOOWULcumTq1xdYsXL6ZTp040a9YMgE6dOjFo0CDatWvHm2++yaGHHgrAk08+yZQpU1JCOo0nnniCyy67jMcff5yhQ4cydvPx6vgl5tAQABtLw7NOFovINjjmmGNYtGgRe+21F+eeey7Tpk0DYOjQoYwfPx6AN954g44dO9KrV6/Ny51yyilMnDgRgOeee44TTjgh+8HXIDEtAgBKlQhEGpSafsG3aFFzeadOtbYAKmrVqhVvv/0206dP59VXX2XIkCHcfPPNnH766QwYMIDbb7+d8ePHM3To0C2W69ChA+3bt2f8+PHss88+tGjRYqvWm2kJSwQbw7MSgYhso5ycHAoKCigoKKBPnz48/PDDnHXWWfTo0YNp06YxYcIEZsyYUWm5IUOGcN555zFmzJjsB12LhCUCtQhEZNvNnTuXRo0abT7sU1RURPfu3YFweOiSSy5hjz32oEuXLpWWPfnkk1m8eDE/+MEP+OKLL7Iad20SkwieegrwnSH3m9BkFBHZSqtWreKCCy5gxYoVNG7cmD333JPCwkIATj31VC666CLuueeeKpdt3bo1V1xxRTbDTVtiEkG4dcCA1jFHIiL11UEHHcTrr79eZVnnzp3ZsGFDpdcXLlxY6bUePXowZ86cug5vmyXmqqExY2DMeW/BNdfEHYqIyA4lWYlgYlsYPTruUEREdiiJSQRAOFmsE8UiIltIViLYqEQgIlJRshJB6UYlAhGRCpKVCHyTEoGISAWJSQSTJ8Pk/+ZD1B+IiMi2aNWq1Xa/x6xZs7jwwgurLV+4cCHjxo1Lu/72Ssx9BOX3kCUm94nIDio/P5/8/Pxqy8sSQVlX1rXV316J2SuOumcjow4ZAy+8EHcoIlJHCgoqP0aNCmVr1lRdXtbVz9Kllcu2VVFREYcddhh9+/bl5JNPZvny5QDMnDmTvn370r9/fy6//PLNg9FMnTqV448/HoBp06aRl5dHXl4eBxxwACtXrmTEiBFMnz6dvLw87rzzzi3qr1q1irPPPps+ffrQt29fJkyYsO2BRxKTCJ4cv4knZ/aAjz+OOxQRaWDOPPNMbrnlFmbPnk2fPn247rrrADj77LN54IEHmDFjBjk5OVUue9ttt3HfffdRVFTE9OnTad68OTfffDMDBw6kqKiISy65ZIv6119/PW3btuW9995j9uzZHHnkkdsdf2IODanDOZGGJ8u9UFeppKSEFStWMGjQIAB+9rOfceqpp7JixQpWrlzJgAEDABg2bBiTJk2qtPzhhx/OpZdeyk9+8hN+9KMfVdlhXaqXX35589gHAO3bt9/ubchoi8DMjjWzuWY238xGVFFuZnZ3VD7bzDI3bJgGpRGRLHL3tOqNGDGChx56iLVr13LYYYfx0Ucf1fq+ZlYXIW6WsURgZjnAfcBgYF9gqJntW6HaYKBX9BgO3J+peNQiEJFMaNu2Le3bt2f69OkAjB07lkGDBtG+fXtat27NG2+8AbDFr/hUn3zyCX369OGKK64gPz+fjz76iNatW7Ny5coq6x9zzDHce++9m+fLzkdsj0y2CA4B5rv7AndfD4wHTqxQ50TgEQ/eANqZ2S4ZicYdGjeBOmhGiUhyrVmzhi5dumx+3HHHHTz88MNcfvnl9O3bl6KiIq699loA/vrXvzJ8+HD69++Pu9O2ih+iI0eOZP/996dfv340b96cwYMH07dvXxo3bky/fv248847t6h/zTXXsHz58s3LvPrqq9u9TZZu82Wr39jsFOBYd/9lNH8GcKi7n59SZxJws7v/XzT/CnCFu8+q8F7DCS0GunXrdtBnn32WkZhFZMf24Ycfss8++8QdRtpWrVq1+b6Dm2++mcWLF3PXXXdlfL1VfU5m9ra7V3kNaiZbBFUdxKqYddKpg7sXunu+u+d37ty5ToITEcm0559/nry8PPbff3+mT5/ONTtoN/iZvGqoGOiaMt8FqDg+Wzp1RETqpSFDhjBkyJC4w6hVJlsEM4FeZtbTzJoCpwPPVqjzLHBmdPXQYUCJuy/OYEwiUs9l6nB2Q7Etn0/GWgTuXmpm5wNTgBxgtLu/b2bnROUPAJOB44D5wBrg7EzFIyL1X25uLsuWLaNjx451fgllQ+DuLFu2jNzc3K1aLmMnizMlPz/fZ82aVXtFEWlwNmzYQHFxMevWrYs7lB1Wbm4uXbp0oUmTJlu8XtPJ4uTcWSwi9V6TJk3o2bNn3GE0OInpa0hERKqmRCAiknBKBCIiCVfvThab2RJgW28t7gQsrcNw6gNtczJom5Nhe7a5u7tXeUduvUsE28PMZlV31ryh0jYng7Y5GTK1zTo0JCKScEoEIiIJl7REUBh3ADHQNieDtjkZMrLNiTpHICIilSWtRSAiIhUoEYiIJFyDTARmdqyZzTWz+WY2oopyM7O7o/LZZnZgHHHWpTS2+SfRts42s9fNrF8ccdal2rY5pd7BZrYxGjWvXktnm82swMyKzOx9M5uW7RjrWhrf7bZm9pyZvRttc73uxdjMRpvZV2Y2p5ryut9/uXuDehC6vP4E2B1oCrwL7FuhznHAC4QR0g4D3ow77ixs8wCgfTQ9OAnbnFLvfwldnp8Sd9xZ+Du3Az4AukXzO8Uddxa2+Srglmi6M/A10DTu2Ldjm78HHAjMqaa8zvdfDbFFcAgw390XuPt6YDxwYoU6JwKPePAG0M7Mdsl2oHWo1m1299fdfXk0+wZhNLj6LJ2/M8AFwATgq2wGlyHpbPMwYKK7fw7g7vV9u9PZZgdaWxigoBUhEZRmN8y64+6vEbahOnW+/2qIiWA3YFHKfHH02tbWqU+2dnt+QfhFUZ/Vus1mthtwMvBAFuPKpHT+znsB7c1sqpm9bWZnZi26zEhnm+8F9iEMc/secJG7b8pOeLGo8/1XQxyPoKphiypeI5tOnfok7e0xsyMIieC7GY0o89LZ5pHAFe6+sYGMZpXONjcGDgKOApoDM8zsDXefl+ngMiSdbf4BUAQcCewB/NPMprv7NxmOLS51vv9qiImgGOiaMt+F8Etha+vUJ2ltj5n1BR4CBrv7sizFlinpbHM+MD5KAp2A48ys1N2fyUqEdS/d7/ZSd18NrDaz14B+QH1NBOls89nAzR4OoM83s0+BvYG3shNi1tX5/qshHhqaCfQys55m1hQ4HXi2Qp1ngTOjs++HASXuvjjbgdahWrfZzLoBE4Ez6vGvw1S1brO793T3Hu7eA3gKOLceJwFI77v9D2CgmTU2sxbAocCHWY6zLqWzzZ8TWkCY2c5Ab2BBVqPMrjrffzW4FoG7l5rZ+cAUwhUHo939fTM7Jyp/gHAFyXHAfGAN4RdFvZXmNl8LdARGRb+QS70e99yY5jY3KOlss7t/aGYvArOBTcBD7l7lZYj1QZp/5+uBMWb2HuGwyRXuXm+7pzazx4ECoJOZFQN/AJpA5vZf6mJCRCThGuKhIRER2QpKBCIiCadEICKScEoEIiIJp0QgIpJwSgSyQ4p6Cy1KefSooe6qOljfGDP7NFrXO2bWfxve4yEz2zeavqpC2evbG2P0PmWfy5yox812tdTPM7Pj6mLd0nDp8lHZIZnZKndvVdd1a3iPMcAkd3/KzI4BbnP3vtvxftsdU23va2YPA/Pc/U811D8LyHf38+s6Fmk41CKQesHMWpnZK9Gv9ffMrFJPo2a2i5m9lvKLeWD0+jFmNiNa9u9mVtsO+jVgz2jZS6P3mmNmF0evtTSz56P+7+eY2ZDo9almlm9mNwPNozgei8pWRc9PpP5Cj1oiPzazHDO71cxmWuhj/tdpfCwziDobM7NDLIwz8e/ouXd0J+4fgSFRLEOi2EdH6/l3VZ+jJFDcfW/roUdVD2AjoSOxIuBpwl3wbaKyToS7KstatKui598CV0fTOUDrqO5rQMvo9SuAa6tY3xii8QqAU4E3CZ23vQe0JHRv/D5wAPBj4C8py7aNnqcSfn1vjimlTlmMJwMPR9NNCb1INgeGA9dErzcDZgE9q4hzVcr2/R04NppvAzSOpo8GJkTTZwH3pix/I/DTaLodoQ+ilnH/vfWI99HgupiQBmOtu+eVzZhZE+BGM/seoeuE3YCdgS9TlpkJjI7qPuPuRWY2CNgX+FfUtUZTwi/pqtxqZtcASwg9tB4FPO2hAzfMbCIwEHgRuM3MbiEcTpq+Fdv1AnC3mTUDjgVec/e10eGovlY+ilpboBfwaYXlm5tZEdADeBv4Z0r9h82sF6EnyibVrP8Y4H/M7LJoPhfoRv3uj0i2kxKB1Bc/IYw+dZC7bzCzhYSd2Gbu/lqUKH4IjDWzW4HlwD/dfWga67jc3Z8qmzGzo6uq5O7zzOwgQn8vN5nZS+7+x3Q2wt3XmdlUQtfJQ4DHy1YHXODuU2p5i7XunmdmbYFJwHnA3YT+dl5195OjE+tTq1negB+7+9x04pVk0DkCqS/aAl9FSeAIoHvFCmbWParzF+CvhOH+3gAON7OyY/4tzGyvNNf5GnBStExLwmGd6Wa2K7DG3R8FbovWU9GGqGVSlfGEjsIGEjpTI3r+TdkyZrZXtM4quXsJcCFwWbRMW+A/UfFZKVVXEg6RlZkCXGBR88jMDqhuHZIcSgRSXzwG5JvZLELr4KMq6hQARWb2b8Jx/LvcfQlhx/i4mc0mJIa901mhu79DOHfwFuGcwUPu/m+gD/BWdIjmauCGKhYvBGaXnSyu4CXCuLQvexh+EcI4ER8A71gYtPxBammxR7G8S+ia+c+E1sm/COcPyrwK7Ft2spjQcmgSxTYnmpeE0+WjIiIJpxaBiEjCKRGIiCScEoGISMIpEYiIJJwSgYhIwikRiIgknBKBiEjC/X8OYDOFudNQFwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(fpr, tpr, 'k--', color='red',label='SVM')\n",
    "plt.plot(fpr1, tpr1, 'k--', color='blue',label='Logistic')\n",
    " \n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "The ROC curve of SVM is sharper in the corner than Logistic Regression. And SVM is faster than Logistic Regression. With the right parametres of $C$ and $\\gamma$, SVM could perform better than Logistic Regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
